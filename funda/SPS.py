import pandas as pd
import numpy as np
import requests
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error
import time
from datetime import datetime, timedelta
from dotenv import load_dotenv
import os
import yfinance as yf
import logging
import dash
from dash import Dash, dcc, html, Input, Output, State
import plotly.express as px
import plotly.graph_objs as go
from plotly.subplots import make_subplots
import dash_bootstrap_components as dbc
import torch
import torch.nn as nn
import json
import warnings
from textblob import TextBlob
from bs4 import BeautifulSoup
from fredapi import Fred
try:
    from scipy.ndimage import gaussian_filter1d
except ImportError:
    # Fallback if scipy is not available
    def gaussian_filter1d(data, sigma):
        return data

# Add parent directory to path to import db module
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from db.core import SessionLocal, engine, Base
from db.models import PerfMetric

# Create tables if they don't exist
Base.metadata.create_all(bind=engine)
from db.models import PerfMetric
from funda.outlier_engine import STRATEGIES
from funda.refresh_outliers import start_refresh_thread, get_refresh_status
from funda.enhanced_features import enhanced_feature_engineering, analyze_feature_importance, select_optimal_features
import threading

# Suppress All-NaN warnings
warnings.filterwarnings("ignore", message="All-NaN slice encountered")

# === HELPER FUNCTIONS ===
def compute_rsi(prices, window=14):
    """Compute Relative Strength Index"""
    delta = prices.diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
    rs = gain / loss
    return 100 - (100 / (1 + rs))

# === INSTITUTIONAL FLOW ANALYSIS FUNCTIONS ===

def compute_institutional_flow_analysis(df):
    """
    Clean function to compute institutional flow analysis with improved accuracy
    """
    df_vol = df.copy()
    
    # Volume Analysis - Check if columns already exist from enhanced features
    if 'Volume_SMA_10' not in df_vol.columns:
        df_vol['Volume_SMA_10'] = df_vol['Volume'].rolling(window=10).mean()
    if 'Volume_SMA_20' not in df_vol.columns:
        df_vol['Volume_SMA_20'] = df_vol['Volume'].rolling(window=20).mean()
    
    # Use existing or create new volume ratios
    if 'Volume_Ratio_10' in df_vol.columns:
        df_vol['Volume_Ratio'] = df_vol['Volume_Ratio_10']  # Use enhanced feature
    else:
        df_vol['Volume_Ratio'] = df_vol['Volume'] / df_vol['Volume_SMA_10']
    
    if 'Volume_Ratio_20' in df_vol.columns:
        df_vol['Volume_Ratio_20'] = df_vol['Volume_Ratio_20']  # Keep existing
    else:
        df_vol['Volume_Ratio_20'] = df_vol['Volume'] / df_vol['Volume_SMA_20']
    
    # Price Volume Trend
    if 'Price_Volume_Trend' not in df_vol.columns:
        price_change = df_vol.get('Price_Change', df_vol['Close'].pct_change())
        df_vol['Price_Volume_Trend'] = price_change * df_vol['Volume_Ratio']
    
    # Price Impact Analysis - Check for existing columns
    if 'Price_Move_Abs' not in df_vol.columns:
        df_vol['Price_Move_Abs'] = abs(df_vol['Close'] - df_vol['Open'])
    if 'Price_Move_Pct' not in df_vol.columns:
        df_vol['Price_Move_Pct'] = df_vol['Price_Move_Abs'] / df_vol['Open']
    if 'High_Low_Range' not in df_vol.columns:
        df_vol['High_Low_Range'] = (df_vol['High'] - df_vol['Low']) / df_vol['Close']
    
    # Price Impact Efficiency
    if 'Price_Impact_Efficiency' not in df_vol.columns:
        df_vol['Price_Impact_Efficiency'] = df_vol['Price_Move_Pct'] / (df_vol['Volume_Ratio'] + 0.1)
    
    # Dollar Volume Analysis - Check for existing columns
    if 'Dollar_Volume' not in df_vol.columns:
        df_vol['Dollar_Volume'] = df_vol['Volume'] * df_vol['Close']
    if 'Dollar_Volume_SMA' not in df_vol.columns:
        df_vol['Dollar_Volume_SMA'] = df_vol['Dollar_Volume'].rolling(20).mean()
    if 'Dollar_Volume_Ratio' not in df_vol.columns:
        df_vol['Dollar_Volume_Ratio'] = df_vol['Dollar_Volume'] / df_vol['Dollar_Volume_SMA']
    
    # Price Impact Classification (Key Feature)
    df_vol['Price_Impact_Percentile'] = df_vol['Price_Impact_Efficiency'].rolling(window=50).rank(pct=True)
    df_vol['High_Price_Impact'] = df_vol['Price_Impact_Percentile'] > 0.8  # Top 20%
    df_vol['Low_Price_Impact'] = df_vol['Price_Impact_Percentile'] < 0.2  # Bottom 20%
    
    # More Strict Institutional Scoring (0-100)
    volume_score = np.clip(df_vol['Volume_Ratio'] * 15, 0, 30)  # Reduced weight
    dollar_score = np.clip(np.log(df_vol['Dollar_Volume_Ratio'] + 1) * 20, 0, 40)  # Increased weight
    efficiency_score = np.clip((1 / (df_vol['Price_Impact_Efficiency'] + 0.1)) * 15, 0, 30)  # Increased weight
    price_consistency = 1 - df_vol['High_Low_Range']
    consistency_score = np.clip(price_consistency * 10, 0, 10)
    
    df_vol['Institutional_Score'] = volume_score + dollar_score + efficiency_score + consistency_score
    
    # More Strict Classification (Higher thresholds)
    df_vol['Likely_Institutional'] = df_vol['Institutional_Score'] > 75  # Increased from 60
    df_vol['Likely_Retail'] = df_vol['Institutional_Score'] < 25  # Decreased from 30
    df_vol['Price_Direction'] = np.where(df_vol['Close'] > df_vol['Open'], 1, -1)
    
    # More Strict Institutional Activity Detection
    df_vol['Institutional_Buying'] = (
        df_vol['Likely_Institutional'] & 
        (df_vol['Price_Direction'] == 1) &
        (df_vol['Price_Move_Pct'] > 0.01) &  # Increased from 0.005 to 0.01 (1%)
        (df_vol['Dollar_Volume'] > df_vol['Dollar_Volume_SMA'] * 2.5) &  # Must be 2.5x average dollar volume
        (df_vol['Volume_Ratio'] > 2.0)  # Must be 2x average volume
    )
    
    df_vol['Institutional_Selling'] = (
        df_vol['Likely_Institutional'] & 
        (df_vol['Price_Direction'] == -1) &
        (df_vol['Price_Move_Pct'] > 0.01) &  # Increased from 0.005 to 0.01 (1%)
        (df_vol['Dollar_Volume'] > df_vol['Dollar_Volume_SMA'] * 2.5) &  # Must be 2.5x average dollar volume
        (df_vol['Volume_Ratio'] > 2.0)  # Must be 2x average volume
    )
    
    # More Strict Capitulation Detection
    df_vol['Capitulation'] = (
        (df_vol['Volume_Ratio_20'] > 4.0) &  # Increased from 3.0 to 4.0
        (df_vol['Price_Direction'] == -1) &
        (df_vol['Price_Move_Pct'] > 0.05) &  # Increased from 0.03 to 0.05 (5%)
        (df_vol['High_Low_Range'] > 0.08) &  # Increased from 0.05 to 0.08 (8%)
        (df_vol['Dollar_Volume'] > df_vol['Dollar_Volume_SMA'] * 3.0)  # Must be 3x average dollar volume
    )
    
    return df_vol

def enhance_chart_with_institutional_flow(fig, df_processed):
    """
    Clean function to enhance chart with institutional flow markers and Price Impact
    """
    try:
        # Update volume bar colors with Price Impact focus
        # REMOVED: Volume coloring logic (no volume chart)
        # Only keep market indicators on the main price chart
        logging.info("Single chart mode - no volume section")
        
        # Add both institutional flow markers and price impact markers
        add_price_impact_markers(fig, df_processed)
        add_institutional_markers(fig, df_processed)
        
        logging.info("Chart enhanced with institutional flow and price impact analysis")
        
    except Exception as e:
        logging.warning(f"Error enhancing chart: {e}")

def add_price_impact_markers(fig, df_processed):
    """
    Add Price Impact markers to chart (High/Low Price Impact) - Cleaner version
    """
    try:
        # High Price Impact markers (Top 20% - Retail/Momentum trading)
        high_impact_dates = df_processed[df_processed.get('High_Price_Impact', False) == True].index
        logging.info(f"Found {len(high_impact_dates)} High Price Impact events")
        if len(high_impact_dates) > 0:
            # Only show the most significant high impact events (top 3 only)
            high_impact_scores = df_processed.loc[high_impact_dates, 'Price_Impact_Efficiency']
            top_3 = high_impact_scores.nlargest(min(3, len(high_impact_scores)))
            significant_high_impact = top_3.index
            
            if len(significant_high_impact) > 0:
                high_impact_prices = df_processed.loc[significant_high_impact, 'High'] * 1.05
                fig.add_trace(create_marker_trace(
                    significant_high_impact, high_impact_prices, 'High Price Impact',
                    'diamond', '#ff00ff', 8, df_processed
                ), row=1, col=1)
        
        # Low Price Impact markers (Bottom 20% - Institutional trading)
        low_impact_dates = df_processed[df_processed.get('Low_Price_Impact', False) == True].index
        logging.info(f"Found {len(low_impact_dates)} Low Price Impact events")
        if len(low_impact_dates) > 0:
            # Only show the most significant low impact events (top 3 only)
            low_impact_scores = df_processed.loc[low_impact_dates, 'Price_Impact_Efficiency']
            bottom_3 = low_impact_scores.nsmallest(min(3, len(low_impact_scores)))
            significant_low_impact = bottom_3.index
            
            if len(significant_low_impact) > 0:
                low_impact_prices = df_processed.loc[significant_low_impact, 'Low'] * 0.95
                fig.add_trace(create_marker_trace(
                    significant_low_impact, low_impact_prices, 'Low Price Impact',
                    'square', '#00ffff', 8, df_processed
                ), row=1, col=1)
        
    except Exception as e:
        logging.warning(f"Error adding price impact markers: {e}")

def add_institutional_markers(fig, df_processed):
    """
    Simplified institutional flow markers - only most important events
    """
    try:
        # Capitulation markers (most important - show all)
        capitulation_dates = df_processed[df_processed.get('Capitulation', False) == True].index
        if len(capitulation_dates) > 0:
            capitulation_prices = df_processed.loc[capitulation_dates, 'Low'] * 0.98
            fig.add_trace(create_marker_trace(
                capitulation_dates, capitulation_prices, 'Capitulation', 
                'triangle-down', '#ff8800', 12, df_processed
            ), row=1, col=1)
        
        # Institutional buying markers (only top 2 most significant)
        inst_buy_dates = df_processed[df_processed.get('Institutional_Buying', False) == True].index
        if len(inst_buy_dates) > 0:
            inst_buy_scores = df_processed.loc[inst_buy_dates, 'Institutional_Score']
            top_inst_buy = inst_buy_scores.nlargest(min(2, len(inst_buy_scores))).index
            
            if len(top_inst_buy) > 0:
                inst_buy_prices = df_processed.loc[top_inst_buy, 'High'] * 1.02
                fig.add_trace(create_marker_trace(
                    top_inst_buy, inst_buy_prices, 'Institutional Buying',
                    'triangle-up', '#00ff00', 10, df_processed
                ), row=1, col=1)
        
        # Institutional selling markers (only top 2 most significant)
        inst_sell_dates = df_processed[df_processed.get('Institutional_Selling', False) == True].index
        if len(inst_sell_dates) > 0:
            inst_sell_scores = df_processed.loc[inst_sell_dates, 'Institutional_Score']
            top_inst_sell = inst_sell_scores.nlargest(min(2, len(inst_sell_scores))).index
            
            if len(top_inst_sell) > 0:
                inst_sell_prices = df_processed.loc[top_inst_sell, 'Low'] * 0.98
                fig.add_trace(create_marker_trace(
                    top_inst_sell, inst_sell_prices, 'Institutional Selling',
                    'triangle-down', '#ff0000', 10, df_processed
                ), row=1, col=1)
                
    except Exception as e:
        logging.warning(f"Error adding institutional markers: {e}")

def create_marker_trace(dates, prices, name, symbol, color, size, df_processed):
    """
    Create standardized marker trace for institutional flow - Cleaner version
    """
    return go.Scatter(
        x=dates,
        y=prices,
        mode='markers',
        marker=dict(
            symbol=symbol,
            size=size,
            color=color,
            line=dict(width=1.5, color='white'),
            opacity=0.8
        ),
        name=name,
        text=[f'{name}<br>{date.strftime("%m/%d")}' for date in dates],  # Shorter text
        hovertemplate='<b>%{text}</b><br>Volume: %{customdata[0]:,}<br>Change: %{customdata[1]:.2%}<br>Score: %{customdata[2]:.1f}<extra></extra>',
        customdata=[[df_processed.loc[date, 'Volume'], 
                     df_processed.loc[date, 'Price_Change'],
                     df_processed.loc[date, 'Institutional_Score']] for date in dates],
        showlegend=True
    )

def generate_volume_analysis_summary(df_processed):
    """
    Generate clean volume analysis summary with Price Impact focus
    """
    try:
        summary_parts = []
        
        # Count events
        capitulation_count = df_processed.get('Capitulation', pd.Series([False] * len(df_processed))).sum()
        inst_buy_count = df_processed.get('Institutional_Buying', pd.Series([False] * len(df_processed))).sum()
        inst_sell_count = df_processed.get('Institutional_Selling', pd.Series([False] * len(df_processed))).sum()
        high_impact_count = df_processed.get('High_Price_Impact', pd.Series([False] * len(df_processed))).sum()
        low_impact_count = df_processed.get('Low_Price_Impact', pd.Series([False] * len(df_processed))).sum()
        
        summary_parts.append("ðŸ“Š VOLUME & PRICE IMPACT ANALYSIS:")
        
        # Price Impact Analysis (Key Focus)
        summary_parts.append(f"ðŸ’¥ HIGH PRICE IMPACT: {high_impact_count} events (Retail/Momentum trading)")
        summary_parts.append(f"ðŸ›ï¸ LOW PRICE IMPACT: {low_impact_count} events (Institutional trading)")
        
        # Capitulation events
        if capitulation_count > 0:
            latest_capitulation = df_processed[df_processed.get('Capitulation', False) == True].index[-1]
            latest_capitulation_vol = df_processed.loc[latest_capitulation, 'Volume']
            latest_capitulation_change = df_processed.loc[latest_capitulation, 'Price_Change']
            summary_parts.append(f"ðŸš¨ CAPITULATION: {capitulation_count} event(s) - Latest: {latest_capitulation.date()} (Vol: {latest_capitulation_vol:,.0f}, Change: {latest_capitulation_change:.2%})")
        
        # Institutional buying (only if significant)
        if inst_buy_count > 0:
            latest_buy = df_processed[df_processed.get('Institutional_Buying', False) == True].index[-1]
            latest_buy_vol = df_processed.loc[latest_buy, 'Volume']
            latest_buy_change = df_processed.loc[latest_buy, 'Price_Change']
            latest_buy_score = df_processed.loc[latest_buy, 'Institutional_Score']
            summary_parts.append(f"ðŸ“ˆ INSTITUTIONAL BUYING: {inst_buy_count} event(s) - Latest: {latest_buy.date()} (Vol: {latest_buy_vol:,.0f}, Change: {latest_buy_change:.2%}, Score: {latest_buy_score:.1f})")
        
        # Institutional selling (only if significant)
        if inst_sell_count > 0:
            latest_sell = df_processed[df_processed.get('Institutional_Selling', False) == True].index[-1]
            latest_sell_vol = df_processed.loc[latest_sell, 'Volume']
            latest_sell_change = df_processed.loc[latest_sell, 'Price_Change']
            latest_sell_score = df_processed.loc[latest_sell, 'Institutional_Score']
            summary_parts.append(f"ðŸ“‰ INSTITUTIONAL SELLING: {inst_sell_count} event(s) - Latest: {latest_sell.date()} (Vol: {latest_sell_vol:,.0f}, Change: {latest_sell_change:.2%}, Score: {latest_sell_score:.1f})")
        
        # Volume statistics
        avg_volume = df_processed['Volume'].mean()
        max_volume = df_processed['Volume'].max()
        max_volume_date = df_processed['Volume'].idxmax()
        max_volume_change = df_processed.loc[max_volume_date, 'Price_Change']
        current_score = df_processed['Institutional_Score'].iloc[-1]
        current_price_impact = df_processed['Price_Impact_Efficiency'].iloc[-1]
        
        summary_parts.append(f"ðŸ“Š VOLUME STATS: Avg: {avg_volume:,.0f}, Max: {max_volume:,.0f} ({max_volume_date.date()}, Change: {max_volume_change:.2%})")
        summary_parts.append(f"ðŸŽ¯ CURRENT INSTITUTIONAL SCORE: {current_score:.1f}/100")
        summary_parts.append(f"âš¡ CURRENT PRICE IMPACT: {current_price_impact:.4f}")
        
        return "\n".join(summary_parts)
        
    except Exception as e:
        logging.warning(f"Error generating volume analysis summary: {e}")
        return "ðŸ“Š VOLUME ANALYSIS: Error generating analysis"

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Log versions
logging.info(f"Dash version: {dash.__version__}")
logging.info(f"dash_bootstrap_components version: {dbc.__version__}")
logging.info(f"PyTorch version: {torch.__version__}")

# Load environment variables
load_dotenv()
FRED_API_KEY = os.getenv("FRED_API_KEY")
ALPHA_VANTAGE_API_KEY = os.getenv("ALPHA_VANTAGE_API_KEY")
NEWS_API_KEY = os.getenv("NEWS_API_KEY")
GROK_API_KEY = os.getenv("GROK_API_KEY")
if not FRED_API_KEY or not ALPHA_VANTAGE_API_KEY:
    raise ValueError("Missing API keys in .env file.")

# Initialize FRED API
fred = Fred(api_key=FRED_API_KEY)

# Define LSTM model for daily predictions
class StockLSTM(nn.Module):
    def __init__(self, input_size, hidden_size=100, num_layers=3, output_size=30, dropout=0.2):
        super(StockLSTM, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout, bidirectional=True)
        self.attention = nn.MultiheadAttention(embed_dim=hidden_size * 2, num_heads=4)
        self.fc = nn.Linear(hidden_size * 2, output_size)
        self.num_layers = num_layers
        self.hidden_size = hidden_size

    def forward(self, x):
        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)
        out, _ = self.lstm(x, (h0, c0))
        out = out.permute(1, 0, 2)
        attn_output, _ = self.attention(out, out, out)
        context = attn_output[-1]
        out = self.fc(context)
        return out

# Define CNN-LSTM model for 1-minute predictions
class CNN_LSTM(nn.Module):
    def __init__(self, price_levels, channels, scalar_features_size, hidden_size=100, num_layers=3, output_size=30, dropout=0.2):
        super(CNN_LSTM, self).__init__()
        self.cnn = nn.Sequential(
            nn.Conv1d(in_channels=channels, out_channels=32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv1d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.AdaptiveMaxPool1d(1)
        )
        self.cnn_residual = nn.Conv1d(channels, 64, kernel_size=1)
        self.cnn_output_size = 64
        self.lstm = nn.LSTM(input_size=self.cnn_output_size + scalar_features_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_size, output_size)
        self.dropout = nn.Dropout(dropout)
        self.num_layers = num_layers
        self.hidden_size = hidden_size

    def forward(self, order_footprint, scalar_features):
        batch_size, seq_length, price_levels, channels = order_footprint.shape
        order_footprint = order_footprint.view(batch_size * seq_length, channels, price_levels)
        cnn_main = self.cnn(order_footprint)
        cnn_residual = self.cnn_residual(order_footprint)
        cnn_residual = nn.functional.adaptive_max_pool1d(cnn_residual, 1)
        cnn_out = cnn_main + cnn_residual
        cnn_out = cnn_out.squeeze(-1)
        cnn_out = cnn_out.view(batch_size, seq_length, self.cnn_output_size)
        if scalar_features.size(-1) != 9:
            raise ValueError(f"Expected scalar_features with last dim 9, got {scalar_features.size(-1)}")
        lstm_input = torch.cat([cnn_out, scalar_features], dim=2)
        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(lstm_input.device)
        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(lstm_input.device)
        lstm_out, _ = self.lstm(lstm_input, (h0, c0))
        lstm_out = lstm_out[:, -1, :]
        lstm_out = self.dropout(lstm_out)
        out = self.fc(lstm_out)
        return out

# Load LSTM models
try:
    daily_model = StockLSTM(input_size=14, output_size=30)  # Updated for new features
    daily_model_path = os.path.join(r"C:\Users\jonel\OneDrive\Desktop\Jonel_Projects\Market_Analysis\funda\model", 'lstm_daily_model.pt')
    daily_model.load_state_dict(torch.load(daily_model_path, weights_only=True))
    daily_model.eval()
    logging.info("Successfully loaded daily LSTM model.")
except Exception as e:
    logging.error(f"Error loading daily LSTM model: {e}")
    raise

try:
    minute_model = CNN_LSTM(price_levels=10, channels=2, scalar_features_size=9, output_size=30)  # Updated for new features
    minute_model_path = os.path.join(r"C:\Users\jonel\OneDrive\Desktop\Jonel_Projects\Market_Analysis\funda\model", 'lstm_1minute_model.pt')
    minute_model.load_state_dict(torch.load(minute_model_path, weights_only=True))
    minute_model.eval()
    logging.info("Successfully loaded 1-minute CNN-LSTM model.")
except Exception as e:
    logging.error(f"Error loading 1-minute CNN-LSTM model: {e}")
    raise

# Function to check for cached data
def load_cached_data(symbol, interval='1d', cache_dir="cache"):
    cache_file = os.path.join(cache_dir, f"{symbol}_{interval.replace(' ', '_')}.csv")
    if os.path.exists(cache_file):
        try:
            df = pd.read_csv(cache_file, index_col=0, parse_dates=True)
            logging.info(f"Loaded cached data for {symbol} (interval={interval})")
            if df.index.tz is not None:
                df.index = df.index.tz_localize(None)
            if set(['Open', 'High', 'Low', 'Close', 'Volume']).issubset(df.columns):
                if interval == '1d' and len(df) < 252:
                    logging.info(f"Cached data for {symbol} has only {len(df)} rows; re-fetching.")
                    return None
                return df
            logging.warning(f"Cached data for {symbol} does not contain expected columns.")
            return None
        except Exception as e:
            logging.warning(f"Error loading cached data for {symbol}: {e}")
    return None

# Function to save data to cache
def save_to_cache(symbol, data, interval='1d', cache_dir="cache"):
    if not os.path.exists(cache_dir):
        os.makedirs(cache_dir)
    cache_file = os.path.join(cache_dir, f"{symbol}_{interval.replace(' ', '_')}.csv")
    if isinstance(data, pd.Series):
        data = data.to_frame(name='Close')
    data.to_csv(cache_file)
    logging.info(f"Saved data for {symbol} (interval={interval}) to {cache_file}")

# Function to fetch stock and sector data
def fetch_yfinance_data(symbol, period='10y', interval='1d'):
    cached_data = load_cached_data(symbol, interval=interval)
    if cached_data is not None:
        return cached_data
    try:
        logging.info(f"Fetching data for {symbol} from yfinance with period={period}, interval={interval}")
        ticker = yf.Ticker(symbol)
        df = ticker.history(period=period, interval=interval)
        if df.empty:
            raise ValueError(f"No data for {symbol}")
        df.index = pd.to_datetime(df.index)
        if df.index.tz is not None:
            df.index = df.index.tz_localize(None)
        expected_columns = ['Open', 'High', 'Low', 'Close', 'Volume']
        df = df[expected_columns]
        min_rows = 60 if interval == '1d' else 30
        if len(df) < min_rows and period != 'max':
            df = ticker.history(period='max', interval=interval)
            if df.empty:
                raise ValueError(f"No data for {symbol} with period='max'")
            df.index = pd.to_datetime(df.index)
            if df.index.tz is not None:
                df.index = df.index.tz_localize(None)
            df = df[expected_columns]
        if interval == '1m':
            df = df.between_time('09:30', '16:00')
        df['Sector_Close'] = df['Close']  # Placeholder; will be updated in ticker analysis
        df['Order_Flow'] = 0  # Placeholder
        logging.info(f"Fetched data for {symbol}. Shape: {df.shape}")
        save_to_cache(symbol, df, interval=interval)
        return df
    except Exception as e:
        logging.error(f"Error fetching {symbol} from yfinance: {e}")
        return None

# Function to fetch order footprint data
def fetch_order_footprint(ticker, period, interval, expected_rows):
    try:
        logging.info(f"Fetching order footprint for {ticker}")
        price_levels = 10
        channels = 2
        footprint = np.random.rand(expected_rows, price_levels, channels) * 100  # Dummy data
        return footprint
    except Exception as e:
        logging.warning(f"Error fetching order footprint for {ticker}: {e}. Returning dummy data.")
        return np.zeros((expected_rows, price_levels, channels))

# Function to fetch Alpha Vantage data
def fetch_alpha_vantage_data(symbol, api_key, delay=12):
    cached_data = load_cached_data(symbol, interval='1d')
    if cached_data is not None:
        return cached_data
    url = f"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol={symbol}&outputsize=full&apikey={api_key}"
    try:
        logging.info(f"Fetching data for {symbol} from Alpha Vantage")
        response = requests.get(url)
        response.raise_for_status()
        data = response.json()
        if 'Time Series (Daily)' not in data:
            raise ValueError(f"No data for {symbol}: {data.get('Information', 'Unknown error')}")
        df = pd.DataFrame.from_dict(data['Time Series (Daily)'], orient='index')
        df.index = pd.to_datetime(df.index)
        df = df.sort_index().astype(float)
        logging.info(f"Fetched data for {symbol} from Alpha Vantage")
        save_to_cache(symbol, df['5. adjusted close'], interval='1d')
        time.sleep(delay)
        return df['5. adjusted close']
    except Exception as e:
        logging.error(f"Error fetching {symbol} from Alpha Vantage: {e}")
        return None

# Function to fetch stock data
def fetch_stock_data(symbol, api_key, period='10y', interval='1d'):
    data = fetch_yfinance_data(symbol, period, interval)
    if data is not None:
        return data
    data = fetch_alpha_vantage_data(symbol, api_key)
    if data is not None:
        return data
    logging.error(f"Failed to fetch data for {symbol}")
    return None

# Function to fetch FRED data
def fetch_fred_data(series_id):
    try:
        logging.info(f"Fetching FRED series {series_id}")
        data = fred.get_series(series_id)
        data.index = pd.to_datetime(data.index)
        if data.index.tz is not None:
            data.index = data.index.tz_localize(None)
        data = data.resample('D').ffill().bfill()
        return data
    except Exception as e:
        logging.error(f"Error fetching FRED series {series_id}: {e}")
        return None

# Compute RSI
def compute_rsi(series, period=14):
    delta = series.diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
    rs = gain / loss
    return 100 - (100 / (1 + rs))

# Compute Realized Volatility
def compute_realized_volatility(df, window=30):
    returns = df['Close'].pct_change().dropna()
    return np.sqrt((returns**2).rolling(window=window).sum())

# Fetch S&P 500 and sector data
sp500 = fetch_stock_data('SPY', ALPHA_VANTAGE_API_KEY)
if sp500 is None:
    logging.error("Failed to fetch S&P 500 proxy (SPY) data.")
    exit()

sector_etfs = {
    'Technology': 'XLK', 'Financials': 'XLF', 'Consumer Discretionary': 'XLY',
    'Industrials': 'XLI', 'Utilities': 'XLU', 'Healthcare': 'XLV',
    'Communication Services': 'XLC', 'Consumer Staples': 'XLP',
    'Materials': 'XLB', 'Real Estate': 'XLRE', 'Energy': 'XLE'
}
sector_data = {}
for sector, ticker in sector_etfs.items():
    sector_data[sector] = fetch_stock_data(ticker, ALPHA_VANTAGE_API_KEY)
    if sector_data[sector] is None:
        logging.warning(f"Failed to fetch data for {sector} ({ticker}). Using zeros.")
        sector_data[sector] = pd.Series(0, index=sp500.index)

# Fetch macroeconomic data
macro_series = {
    'GDP': 'A191RL1Q225SBEA', 'Inflation': 'CPIAUCSL', 'Unemployment': 'UNRATE',
    'Interest Rate': 'FEDFUNDS', 'Consumer Confidence': 'UMCSENT', 'VIX': 'VIXCLS',
    '10Y Treasury': 'DGS10', '2Y Treasury': 'DGS2'
}
macro_data = {key: fetch_fred_data(series_id) for key, series_id in macro_series.items()}

# Combine data
common_index = sp500.index
data_dict = {'SP500': sp500['Close'] if isinstance(sp500, pd.DataFrame) else sp500}
for sector in sector_data:
    data_dict[f"{sector}_Close"] = sector_data[sector]['Close'] if isinstance(sector_data[sector], pd.DataFrame) else sector_data[sector]
for key in macro_data:
    if macro_data[key] is not None:
        data_dict[key] = macro_data[key].reindex(common_index, method='ffill').bfill()
    else:
        logging.warning(f"Macro data for {key} is None. Filling with zeros.")
        data_dict[key] = pd.Series(0, index=common_index)

data = pd.DataFrame(data_dict).dropna()

# Feature Engineering
data['SP500_Returns'] = data['SP500'].pct_change(periods=5).shift(-5)
for sector in sector_data:
    data[f"{sector}_Returns"] = data[f"{sector}_Close"].pct_change(periods=5).shift(-5)
data['SP500_MA20'] = data['SP500'].rolling(window=20).mean()
data['SP500_RSI'] = compute_rsi(data['SP500'], 14)
data['Yield_Spread'] = data['10Y Treasury'] - data['2Y Treasury']
data['GDP_Lag'] = data['GDP'].shift(1)
data['Inflation_Lag'] = data['Inflation'].shift(1)
data = data.dropna()

# Define features
features = ['SP500_MA20', 'SP500_RSI', 'GDP_Lag', 'Inflation_Lag', 'Unemployment',
            'Interest Rate', 'Consumer Confidence', 'VIX', 'Yield_Spread']
X = data[features]
nan_columns = X.columns[X.isna().all()]
if not nan_columns.empty:
    logging.warning(f"Columns with all NaN values: {nan_columns.tolist()}")
    X = X.drop(columns=nan_columns)

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X = pd.DataFrame(X_scaled, index=X.index, columns=X.columns)

# Define targets
y_market = (data['SP500_Returns'] > 0.01).astype(int)
y_sector = {sector: data[f"{sector}_Returns"] for sector in sector_data}

# Train-test split
train_size = int(len(X) * 0.8)
X_train, X_test = X[:train_size], X[train_size:]
y_market_train, y_market_test = y_market[:train_size], y_market[train_size:]
y_sector_train = {sector: y_sector[sector][:train_size] for sector in y_sector}
y_sector_test = {sector: y_sector[sector][train_size:] for sector in y_sector}

# Train models
market_model = RandomForestClassifier(n_estimators=100, random_state=42)
market_model.fit(X_train, y_market_train)
sector_models = {}
for sector in y_sector:
    sector_models[sector] = RandomForestRegressor(n_estimators=100, random_state=42)
    sector_models[sector].fit(X_train, y_sector_train[sector])

# Evaluate models
market_pred = market_model.predict(X_test)
market_accuracy = accuracy_score(y_market_test, market_pred)
logging.info(f"Market Direction Accuracy: {market_accuracy:.2f}")

sector_mse = {}
for sector in y_sector:
    sector_pred = sector_models[sector].predict(X_test)
    sector_mse[sector] = mean_squared_error(y_sector_test[sector], sector_pred)
    logging.info(f"{sector} MSE: {sector_mse[sector]:.4f}")

# Latest S&P 500 prediction
latest_data = X.iloc[-1:].copy()
market_direction = market_model.predict(latest_data)[0]
market_confidence = market_model.predict_proba(latest_data)[0][1] if market_direction == 1 else market_model.predict_proba(latest_data)[0][0]
market_direction_label = "Bullish" if market_direction == 1 else "Bearish"
logging.info(f"Predicted Market Direction: {market_direction_label} (Confidence: {market_confidence:.2%})")

sector_predictions = {}
for sector in y_sector:
    sector_predictions[sector] = sector_models[sector].predict(latest_data)[0]
    logging.info(f"Predicted {sector} 5-Day Return: {sector_predictions[sector]:.4f}")

# Feature importance
feature_importance = pd.DataFrame({
    'Feature': features,
    'Importance': market_model.feature_importances_
}).sort_values(by='Importance', ascending=False)

# Prepare data for dashboard
sp500_df = pd.DataFrame({'Date': sp500.index, 'SP500': sp500['Close'] if isinstance(sp500, pd.DataFrame) else sp500})
sector_historical = pd.DataFrame(index=common_index)
for sector in sector_data:
    sector_historical[sector] = sector_data[sector]['Close'] if isinstance(sector_data[sector], pd.DataFrame) else sector_data[sector]
sector_historical = sector_historical.reset_index().rename(columns={'index': 'Date'})
sector_historical_melted = sector_historical.melt(id_vars='Date', var_name='Sector', value_name='Price')
sector_pred_df = pd.DataFrame.from_dict(sector_predictions, orient='index', columns=['Predicted_5Day_Return'])
sector_pred_df.reset_index(inplace=True)
sector_pred_df.rename(columns={'index': 'Sector'}, inplace=True)
feature_importance_df = feature_importance.copy()

# Initialize Dash app
app = Dash(
    __name__, 
    external_stylesheets=[dbc.themes.BOOTSTRAP],
    suppress_callback_exceptions=True,
    # Fix for chunk loading errors and HTTP protocol issues
    serve_locally=True,
    # Prevent caching issues
    assets_folder='assets',
    assets_url_path='assets'
)

# Custom CSS for dark background
app.index_string = '''
<!DOCTYPE html>
<html>
    <head>
        {%metas%}
        <title>{%title%}</title>
        {%favicon%}
        {%css%}
        <style>
            body { background-color: #000 !important; }
            .container, .row, .col, .dash-table-container, .dash-table, .dash-spreadsheet-container {
                background-color: #000 !important;
                color: #fff !important;
            }
            .card, .card-body, .navbar, .modal-content, .dropdown-menu {
                background-color: #111 !important;
                color: #fff !important;
            }
            .form-control, .input-group-text, .custom-select, .dropdown-item {
                background-color: #222 !important;
                color: #fff !important;
            }
            .text-dark { color: #fff !important; }
            .text-primary { color: #39FF14 !important; }
            .bg-light { background-color: #222 !important; }
            /* Custom for dropdown */
            .Select-control, .Select-menu-outer, .Select-menu, .Select-option, .Select-placeholder, .Select--single {
                background-color: #222 !important;
                color: #fff !important;
                border-radius: 8px !important;
                font-size: 18px !important;
            }
            .Select-placeholder { color: #bbb !important; }
            .Select-arrow-zone { color: #fff !important; }
            /* Custom for dcc.Input */
            input[type="text"] {
                background-color: #222 !important;
                color: #fff !important;
                border: 1px solid #444 !important;
                border-radius: 8px !important;
                font-size: 18px !important;
                padding: 8px 16px !important;
                box-shadow: 0 0 8px #111;
            }
            input[type="text"]::placeholder {
                color: #bbb !important;
                font-style: italic;
                opacity: 1;
            }
            /* Custom for dcc.Dropdown */
            .dash-dropdown, .dash-dropdown * {
                background-color: #222 !important;
                color: #fff !important;
                border-radius: 8px !important;
                font-size: 18px !important;
            }
            .dotdigital-title-small {
                font-family: 'EnhancedDotDigital7', monospace, sans-serif;
                color: #FF7043 !important;
                font-size: 24x !important; /* or your preferred size */
                letter-spacing: 0.05em;
                text-shadow: 0 0 8px #FF7043, 0 0 2px #fff;
                text-transform: uppercase;
                line-height: 1.1;
            }
        </style>
    </head>
    <body>
        {%app_entry%}
        <footer>
            {%config%}
            {%scripts%}
            {%renderer%}
        </footer>
    </body>
</html>
'''

# Dashboard layout
app.layout = dbc.Container([
    dbc.Row([
        dbc.Col(html.H1("BILLIONS", className="dotdigital-title text-center mb-4")),
        dbc.Col(html.P("Grow your wealth with real-time market insights, sector trends, and custom stock forecastsâ€”powered by outlier detection.", 
                       className="neon-green text-center mb-4"), width=12),
        dbc.Col(html.P(f"Last Updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", 
                       className="neon-green text-center mb-4"), width=12)
    ]),
    # Refresh button row
    dbc.Row([
        dbc.Col([
            dbc.Button(
                "ðŸ”„ Refresh Outlier Data", 
                id="refresh-button",
                color="success",
                className="me-2",
                disabled=False
            ),
            dbc.Spinner(
                html.Div(id="refresh-status", children="Ready to refresh data"),
                id="refresh-spinner",
                color="primary"
            )
        ], width=12, className="text-center mb-4")
    ]),
    dbc.Row([
        dbc.Col([
            html.H4("Custom Ticker Analysis", className="dotdigital-title-small text-left mb-2"),
            html.Div([
                # Row for input and button
                html.Div([
                    dcc.Input(
                        id='ticker-input',
                        type='text',
                        placeholder='Enter NASDAQ ticker',
                        style={
                            'width': '200px',
                            'marginRight': '10px',
                            'backgroundColor': '#222',
                            'color': '#fff',
                            'border': '1px solid #444',
                            'borderRadius': '8px',
                            'fontSize': '18px',
                            'padding': '8px 16px',
                            'boxShadow': '0 0 8px #111'
                        }
                    ),
                    html.Button('Generate Chart', id='generate-button', n_clicks=0, style={
                        'backgroundColor': '#222',
                        'color': '#39FF14',
                        'border': '1px solid #39FF14',
                        'borderRadius': '8px',
                        'fontSize': '18px',
                        'padding': '8px 16px',
                        'marginLeft': '10px',
                        'verticalAlign': 'middle',
                        'boxShadow': '0 0 8px #39FF14'
                    })
                ], style={'display': 'flex', 'alignItems': 'center', 'marginBottom': '0'}),
                # Dropdown below
                dcc.Dropdown(
                    id='timeframe-dropdown',
                    options=[
                        {'label': 'Daily', 'value': 'daily'},
                        {'label': '1-Minute', 'value': '1-minute'},
                    ],
                    value='daily',
                    style={
                        'width': '100px',
                        'display': 'block',
                        'marginTop': '10px',
                        'marginBottom': '10px',
                        'backgroundColor': '#222',
                        'color': '#fff',
                        'borderRadius': '8px',
                        'fontSize': '12px'
                    }
                )
            ], style={'display': 'flex', 'flexDirection': 'column', 'alignItems': 'flex-start', 'gap': '0'}),
            html.Div(id='ticker-error', style={'color': 'red'})
        ], width=12, className="mb-4")
    ]),
    dbc.Row([
        dbc.Col([
            html.Div(
                id='ticker-chart-area',
                style={
                    'height': '500px',
                    'backgroundColor': '#222',
                    'borderRadius': '12px',
                    'marginBottom': '24px',
                    'position': 'relative',
                    'overflow': 'hidden',
                    'width': '100%'
                },
                children=[
                    html.Div([
                        html.Img(
                            src='/assets/nanakorobi_yaoki.png',
                            style={
                                'height': '180px',
                                'marginBottom': '24px',
                                'opacity': '0.5',
                                'display': 'block'
                            }
                        ),
                        html.Div(
                            'ä¸ƒ è»¢ å…« èµ·',
                            style={
                                'color': '#39FF14',
                                'fontSize': '38px',
                                'textAlign': 'center',
                                'fontFamily': 'Arial, sans-serif',
                                'letterSpacing': '0.05em'
                            }
                        )
                    ], style={
                        'position': 'absolute',
                        'top': '50%',
                        'left': '50%',
                        'transform': 'translate(-50%, -50%)'
                    })
                ]
            )
        ], width=12, className="mb-4")
    ]),
    dbc.Row([
        dbc.Col([
            html.H3(id='market-prediction', className="text-center", style={'color': '#39FF14', 'fontWeight': 'bold'}),
            html.P("Key Drivers:", style={'color': '#fff', 'fontSize': '20px', 'marginBottom': '8px'}),
            html.Ul(id='key-drivers', style={'color': '#fff', 'fontSize': '18px'}),
            html.H4("Economic News", style={'color': '#39FF14', 'marginTop': '24px'}),
            html.Div(id='news-section'),
            html.H4("Ticker Analysis", style={'color': '#39FF14', 'marginTop': '24px'}),
            html.Div(id='ticker-analysis-section', style={'color': '#fff', 'fontSize': '16px', 'marginBottom': '16px'}),
            html.H4("Hype/Promotion", style={'color': '#39FF14', 'marginTop': '24px'}),
            html.Div(id='hype-section', style={'color': '#fff', 'fontSize': '16px', 'marginBottom': '16px'}),
            html.H4("Caveat Emptor/Prohibited Alarm", style={'color': '#FF4C4C', 'marginTop': '24px'}),
            html.Div(id='alarm-section', style={'color': '#FF4C4C', 'fontSize': '16px', 'marginBottom': '16px'})
        ], width=3, className="p-3", style={
            'height': '100vh',
            'overflow': 'auto',
            'backgroundColor': '#000',
            'borderRadius': '12px'
        }),
        dbc.Col([
            # 1. Performance Scatter Plot (moved to top)
            dbc.Row([
                dbc.Col([
                    dcc.Dropdown(
                        id='strategy-dropdown',
                        options=[
                            {'label': 'Scalp', 'value': 'scalp'},
                            {'label': 'Swing', 'value': 'swing'},
                            {'label': 'Longterm', 'value': 'longterm'}
                        ],
                        value='scalp',
                        clearable=False,
                        style={'width': '200px', 'marginBottom': '16px', 'backgroundColor': '#222', 'color': '#fff'}
                    ),
                    dcc.Graph(
                        id='feature-outlier-scatter',
                        config={'displayModeBar': True},
                        style={'height': '500px', 'width': '1200px', 'margin': '0 auto'}
                    )
                ], style={'backgroundColor': '#222', 'borderRadius': '12px', 'padding': '16px'})
            ], className="mb-4"),
            
            # 2. NASDAQ Historical Data (replacing S&P 500)
            dbc.Row([dbc.Col(dcc.Graph(id='nasdaq-chart', figure={}), width=12)], className="mb-4"),
            
            # 3. Gold Futures Historical Data (replacing Sector ETF)
            dbc.Row([dbc.Col(dcc.Graph(id='gold-futures-chart', figure={}), width=12)], className="mb-4"),
            
            # 4. Bitcoin Futures Historical Data (replacing Predicted Sector Returns)
            dbc.Row([dbc.Col(dcc.Graph(id='bitcoin-futures-chart', figure={}), width=12)], className="mb-4")
        ], width=9)
    ]),
    # Logo and partnership text at bottom left
    html.Div([
        html.Img(
            src="/assets/logo.png",
            style={
                'height': '60px',
                'marginRight': '12px',
                'verticalAlign': 'middle'
            }
        ),
        html.Span(
            "Partnership with Kumpooni",
            style={
                'color': '#fff',
                'fontSize': '10px',
                'verticalAlign': 'middle',
                'fontFamily': 'Arial, sans-serif',
                'textShadow': '0 0 8px #fff, 0 0 2px #fff'
            }
        )
    ], style={
        'position': 'fixed',
        'left': '20px',
        'bottom': '20px',
        'zIndex': '1000',
        'display': 'flex',
        'alignItems': 'center',
        'background': 'rgba(0,0,0,0.5)',
        'padding': '8px 16px',
        'borderRadius': '12px'
    }),
    # Interval component for progress updates
    dcc.Interval(
        id='interval-component',
        interval=2000,  # Update every 2 seconds
        n_intervals=0
    )
], fluid=True)

# Callback to update dashboard with new chart layout
@app.callback(
    [
        Output('market-prediction', 'children'),
        Output('key-drivers', 'children'),
        Output('nasdaq-chart', 'figure'),
        Output('gold-futures-chart', 'figure'),
        Output('bitcoin-futures-chart', 'figure'),
        Output('news-section', 'children')
    ],
    Input('generate-button', 'n_clicks'),
    State('ticker-input', 'value')
)
def update_sp500_dashboard(n_clicks, ticker):
    try:
        # Check if required variables are available
        if 'market_direction_label' not in globals() or 'market_confidence' not in globals():
            return html.Div("Loading market data...", style={'color': '#39FF14'}), html.Div("Loading..."), {}, {}, {}, html.Div("Loading news...")
        
        market_pred_text = html.Div(
            f"Market Prediction: {market_direction_label} (Confidence: {market_confidence:.2%})",
            style={'fontFamily': 'EnhancedDotDigital7', 'fontSize': '24px'}
        )
        key_drivers = [html.Li(
            f"{row['Feature']}: {row['Importance']:.2%}",
            style={'fontFamily': 'EnhancedDotDigital7', 'fontSize': '15px'}
        ) for _, row in feature_importance_df.iterrows()]
        # Generate new charts for the updated dashboard
        # 1. NASDAQ 100 Chart (replacing S&P 500)
        try:
            nasdaq_data = yf.download('^NDX', start='2020-01-01', end=None, progress=False)  # NASDAQ 100 Index
            logging.info("Successfully loaded NASDAQ 100 data using symbol: ^NDX")
            nasdaq_df = nasdaq_data.reset_index()
            # Fix multi-level column names
            nasdaq_df.columns = ['Date'] + [col[0] if isinstance(col, tuple) else col for col in nasdaq_df.columns[1:]]
            nasdaq_fig = px.bar(nasdaq_df, x='Date', y='Close', title='NASDAQ 100 (^NDX) Historical Data', template='none')
            nasdaq_fig.update_layout(
            plot_bgcolor='black',
            paper_bgcolor='black', 
            font_color='#fff',
            xaxis=dict(color='#fff'),
            yaxis=dict(color='#fff'),
            font_family='EnhancedDotDigital7',
            font_size=24
        )
            # Set neon green color for bars
            nasdaq_fig.update_traces(marker_color='#39FF14')
        except Exception as e:
            logging.error(f"Error fetching NASDAQ 100 data: {e}")
            nasdaq_fig = px.line(title='NASDAQ 100 Data Unavailable', template='none')
            nasdaq_fig.update_layout(plot_bgcolor='black', paper_bgcolor='black', font_color='#fff')
        
        # 2. Gold Futures Chart (replacing Sector ETF)
        try:
            # Try multiple gold symbols
            gold_symbols = ['GC=F', 'GOLD', 'GLD', 'IAU']
            gold_data = None
            gold_symbol_used = None
            
            for symbol in gold_symbols:
                try:
                    gold_data = yf.download(symbol, start='2020-01-01', end=None, progress=False)
                    if not gold_data.empty:
                        gold_symbol_used = symbol
                        break
                except:
                    continue
            
            if gold_data is not None and not gold_data.empty:
                logging.info(f"Successfully loaded Gold data using symbol: {gold_symbol_used}")
                gold_df = gold_data.reset_index()
                # Fix multi-level column names
                gold_df.columns = ['Date'] + [col[0] if isinstance(col, tuple) else col for col in gold_df.columns[1:]]
                gold_fig = px.bar(gold_df, x='Date', y='Close', title=f'Gold ({gold_symbol_used}) Historical Data', template='none')
                gold_fig.update_layout(
            plot_bgcolor='black',
            paper_bgcolor='black',
            font_color='#fff',
            xaxis=dict(color='#fff'),
                    yaxis=dict(color='#fff'),
                    font_family='EnhancedDotDigital7',
                    font_size=24
                )
                # Set neon green color for bars
                gold_fig.update_traces(marker_color='#39FF14')
            else:
                raise Exception("No gold data available from any symbol")
        except Exception as e:
            logging.error(f"Error fetching Gold data: {e}")
            gold_fig = px.line(title='Gold Futures Data Unavailable', template='none')
            gold_fig.update_layout(plot_bgcolor='black', paper_bgcolor='black', font_color='#fff')
        
        # 3. Bitcoin Futures Chart (replacing Predicted Sector Returns)
        try:
            # Try multiple bitcoin symbols
            bitcoin_symbols = ['BTC-USD', 'BTC=F', 'BITCOIN-USD', 'BTC1-USD']
            bitcoin_data = None
            bitcoin_symbol_used = None
            
            for symbol in bitcoin_symbols:
                try:
                    bitcoin_data = yf.download(symbol, start='2020-01-01', end=None, progress=False)
                    if not bitcoin_data.empty:
                        bitcoin_symbol_used = symbol
                        break
                except:
                    continue
            
            if bitcoin_data is not None and not bitcoin_data.empty:
                logging.info(f"Successfully loaded Bitcoin data using symbol: {bitcoin_symbol_used}")
                bitcoin_df = bitcoin_data.reset_index()
                # Fix multi-level column names
                bitcoin_df.columns = ['Date'] + [col[0] if isinstance(col, tuple) else col for col in bitcoin_df.columns[1:]]
                bitcoin_fig = px.bar(bitcoin_df, x='Date', y='Close', title=f'Bitcoin ({bitcoin_symbol_used}) Historical Data', template='none')
                bitcoin_fig.update_layout(
            plot_bgcolor='black',
            paper_bgcolor='black',
            font_color='#fff',
            xaxis=dict(color='#fff'),
                    yaxis=dict(color='#fff'),
                    font_family='EnhancedDotDigital7',
                    font_size=24
                )
                # Set neon green color for bars
                bitcoin_fig.update_traces(marker_color='#39FF14')
            else:
                raise Exception("No bitcoin data available from any symbol")
        except Exception as e:
            logging.error(f"Error fetching Bitcoin data: {e}")
            bitcoin_fig = px.line(title='Bitcoin Data Unavailable', template='none')
            bitcoin_fig.update_layout(plot_bgcolor='black', paper_bgcolor='black', font_color='#fff')
        market_direction_text = f"Market Direction: {market_direction_label} (Confidence: {market_confidence:.2%})"
        explanation = [
            html.Li(f"Economic Growth: GDP at {data['GDP'].iloc[-1]:.2f}% supports {'bullish' if market_direction_label == 'Bullish' else 'bearish'} markets."),
            html.Li(f"Unemployment: {data['Unemployment'].iloc[-1]:.2f}% indicates {'strong' if data['Unemployment'].iloc[-1] < 5 else 'weak'} labor markets."),
            html.Li(f"Inflation: CPI at {data['Inflation'].iloc[-1]/100:.2f}% suggests {'stable' if data['Inflation'].iloc[-1]/100 < 3 else 'high'} price pressures."),
            html.Li(f"VIX: {data['VIX'].iloc[-1]:.2f}, indicating {'low' if data['VIX'].iloc[-1] < 20 else 'high'} volatility.")
        ]
        key_driver_text = f"Key Driver: {feature_importance.iloc[0]['Feature']} ({feature_importance.iloc[0]['Importance']:.2%})"
        risk_factors_text = f"Risk Factors: Federal Funds Rate at {data['Interest Rate'].iloc[-1]:.2f}% may impact valuations."
        sector_performance = [html.Li(f"{sector}: {return_pred:.4f}") for sector, return_pred in sector_predictions.items()]
        investors_insights = [
            html.Li("Overweight: Sectors with positive returns (e.g., Utilities)."),
            html.Li("Underweight: Sectors with negative returns (e.g., Consumer Discretionary).")
        ]
        traders_insights = [
            html.Li("Monitor Fed signalsâ€”rate hikes above 5% could shift outlook."),
            html.Li("Consider shorts in underperforming sectors.")
        ]
        long_term_view = f"Sustained GDP growth (>3%) could {'extend bullish trend' if market_direction_label == 'Bullish' else 'mitigate bearish pressures'} into Q3 2025."

        # Fetch and classify news
        # NOTE: Disabled LLM for economic news to avoid rate limits
        # LLM is still used for ticker-specific analysis which is more valuable
        
        try:
            # Get sector-specific news if ticker is provided
            if ticker and ticker.strip():
                sector_info = get_ticker_sector_info(ticker.strip().upper())
                sector = sector_info['sector']
                sector_keywords = sector_info['keywords']
                
                logging.info(f"Fetching {sector} sector news for ticker {ticker.upper()}")
                logging.info(f"Sector keywords: {sector_keywords}")
                
                # Fetch sector-specific news using the keywords
                articles = fetch_comprehensive_news(NEWS_API_KEY, ticker=ticker, use_fallback=False)
                
                # If no ticker-specific news, try sector-specific news
                if len(articles) < 3:
                    logging.info(f"Not enough ticker-specific news, trying sector-specific news for {sector}")
                    sector_articles = fetch_economic_news(NEWS_API_KEY, query=sector_keywords, page_size=15)
                    if sector_articles:
                        for article in sector_articles:
                            article['source'] = 'NewsAPI'
                        articles.extend(sector_articles)
                        logging.info(f"Added {len(sector_articles)} sector-specific articles")
                
                news_context = f"{sector} Sector News"
            else:
                # General economic news
                articles = fetch_comprehensive_news(NEWS_API_KEY)
                news_context = "General Economic News"
            
            # Use keyword-based classification for economic news (fast, no API cost)
            good, bad, hidden = classify_news(articles)
            logging.info(f"{news_context} classification (keyword): Good={len(good)}, Bad={len(bad)}, Hidden={len(hidden)}")
            
            # If all categories are empty, show informative message
            if not good and not bad and not hidden:
                logging.warning("No articles classified into any category - possible API issue or filtering too strict")
        except Exception as e:
            logging.error(f"Error fetching comprehensive news: {e}")
            articles = [{'title': 'News temporarily unavailable', 'url': '', 'description': '', 'source': 'System'}]
            good, bad, hidden = [], [], []
        
        # Build news display with sector context and verbose information
        news_title = f"Economic News ({news_context})" if 'news_context' in locals() else "Economic News"
        news_children = [
            html.Div([
                html.H4(news_title, style={'color': '#fff', 'marginBottom': '10px', 'fontSize': '18px'}),
                html.P(f"ðŸ“Š Analysis based on {len(articles)} articles from multiple sources", 
                      style={'color': '#888', 'fontSize': '12px', 'marginBottom': '15px'})
            ]),
            html.Div([
                html.Strong("âœ… Good News:", style={'color': '#39FF14', 'fontSize': '16px'}),
                html.Ul([html.Li([
                    html.A(a['title'], href=a['url'], target="_blank", style={'color': '#39FF14'}),
                    html.Br(),
                    html.Span(f"Source: {a.get('source', 'Unknown')} | ", style={'color': '#888', 'fontSize': '11px'}),
                    html.Span(f"Published: {a.get('publishedAt', 'Unknown')[:10] if a.get('publishedAt') else 'Unknown'}", 
                             style={'color': '#888', 'fontSize': '11px'})
                ]) for a in good[:8]]) if good else html.P("ðŸ” No positive market news found in current analysis.", 
                                                           style={'color': '#888', 'fontSize': '14px', 'fontStyle': 'italic'})
            ], style={'marginBottom': '15px'}),
            html.Div([
                html.Strong("âŒ Bad News:", style={'color': '#FF4C4C', 'fontSize': '16px'}),
                html.Ul([html.Li([
                    html.A(a['title'], href=a['url'], target="_blank", style={'color': '#FF4C4C'}),
                    html.Br(),
                    html.Span(f"Source: {a.get('source', 'Unknown')} | ", style={'color': '#888', 'fontSize': '11px'}),
                    html.Span(f"Published: {a.get('publishedAt', 'Unknown')[:10] if a.get('publishedAt') else 'Unknown'}", 
                             style={'color': '#888', 'fontSize': '11px'})
                ]) for a in bad[:8]]) if bad else html.P("ðŸ” No negative market news found in current analysis.", 
                                                         style={'color': '#888', 'fontSize': '14px', 'fontStyle': 'italic'})
            ], style={'marginBottom': '15px'}),
            html.Div([
                html.Strong("ðŸ’¡ Hidden Edge:", style={'color': '#FFD700', 'fontSize': '16px'}),
                html.Ul([html.Li([
                    html.A(a['title'], href=a['url'], target="_blank", style={'color': '#FFD700'}),
                    html.Br(),
                    html.Span(f"Source: {a.get('source', 'Unknown')} | ", style={'color': '#888', 'fontSize': '11px'}),
                    html.Span(f"Published: {a.get('publishedAt', 'Unknown')[:10] if a.get('publishedAt') else 'Unknown'}", 
                             style={'color': '#888', 'fontSize': '11px'})
                ]) for a in hidden[:8]]) if hidden else html.P("ðŸ” No hidden edge opportunities found in current analysis.", 
                                                               style={'color': '#888', 'fontSize': '14px', 'fontStyle': 'italic'})
            ], style={'marginBottom': '10px'}),
            html.Div([
                html.P(f"ðŸ“ˆ Total Articles Analyzed: {len(articles)} | ðŸŽ¯ Sector Focus: {news_context}", 
                      style={'color': '#666', 'fontSize': '10px', 'marginTop': '10px', 'textAlign': 'center'})
            ])
        ]

        return (market_pred_text, key_drivers, nasdaq_fig, gold_fig, bitcoin_fig, news_children)
    
    except Exception as e:
        logging.error(f"Error in update_sp500_dashboard callback: {e}")
        error_message = html.Div(f"Error loading dashboard: {str(e)}", style={'color': '#FF4C4C'})
        return error_message, html.Div("Error loading data"), {}, {}, {}, html.Div("Error loading news")

# Simple cache for Grok API responses to avoid rate limiting
_grok_cache = {}
_grok_cache_timestamp = {}
CACHE_DURATION = 600  # 10 minutes (increased to reduce API calls)

# Add Grok API call function
def call_grok_api(prompt, api_key=GROK_API_KEY, use_cache=True):
    """
    Call Grok API for LLM-based analysis with rate limiting, retry logic, and caching
    """
    if not api_key:
        logging.warning("GROK_API_KEY not set, skipping LLM analysis")
        return None
    
    # Check cache first
    if use_cache:
        import hashlib
        import time
        
        prompt_hash = hashlib.md5(prompt.encode()).hexdigest()
        current_time = time.time()
        
        if prompt_hash in _grok_cache:
            cache_time = _grok_cache_timestamp.get(prompt_hash, 0)
            if current_time - cache_time < CACHE_DURATION:
                logging.info(f"Using cached Grok response (age: {int(current_time - cache_time)}s)")
                return _grok_cache[prompt_hash]
            else:
                # Cache expired
                logging.info("Cache expired, fetching fresh response")
    
    try:
        import time
        url = "https://api.x.ai/v1/chat/completions"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }
        data = {
            "messages": [
                {"role": "system", "content": "You are a financial news and stock analysis assistant."},
                {"role": "user", "content": prompt}
            ],
                "model": "grok-beta",
            "stream": False,
                "temperature": 0.3
            }
            
            # Try with retry on rate limit
        max_retries = 2
        for attempt in range(max_retries):
            try:
                response = requests.post(url, headers=headers, json=data, timeout=15)
                
                # Handle rate limiting
                if response.status_code == 429:
                    if attempt < max_retries - 1:
                        wait_time = 2 ** attempt  # Exponential backoff: 1s, 2s
                        logging.warning(f"Rate limited, waiting {wait_time}s before retry...")
                        time.sleep(wait_time)
                        continue
                    else:
                        logging.error("Rate limit exceeded, max retries reached")
                        return None
                
                response.raise_for_status()
                result = response.json()["choices"][0]["message"]["content"]
                
                # Cache the successful response
                if use_cache:
                    _grok_cache[prompt_hash] = result
                    _grok_cache_timestamp[prompt_hash] = time.time()
                    logging.info("Cached Grok API response")
                
                return result
                
            except requests.exceptions.HTTPError as e:
                if e.response.status_code == 429 and attempt < max_retries - 1:
                    continue
                else:
                    raise
        
        return None
        
    except requests.exceptions.Timeout:
        logging.error("Grok API timeout after 15 seconds")
        return None
    except requests.exceptions.RequestException as e:
        logging.error(f"Grok API request error: {e}")
        return None
    except Exception as e:
        logging.error(f"Grok API error: {e}")
        return None

def format_news_for_prompt(news_articles):
    """
    Format news articles for LLM prompt
    """
    lines = []
    for i, a in enumerate(news_articles[:10], 1):  # Limit to 10 articles
        title = a.get('title', '')
        desc = a.get('description', '')
        source = a.get('source', 'Unknown')
        lines.append(f"{i}. [{source}] {title}")
        if desc:
            lines.append(f"   {desc[:150]}...")
    return '\n'.join(lines)

def classify_news_with_llm(articles):
    """
    Use LLM to classify news articles into Good, Bad, and Hidden Edge categories
    More accurate than keyword matching
    """
    if not articles or len(articles) == 0:
        return None
    
    news_text = format_news_for_prompt(articles)
    
    prompt = f"""Classify these financial news articles into three categories:

NEWS ARTICLES:
{news_text}

Categories:
- GOOD NEWS: Positive market developments (rallies, growth, earnings beats, bullish signals)
- BAD NEWS: Negative market developments (crashes, declines, bearish signals, risks)
- HIDDEN EDGE: Overlooked opportunities, unexpected developments, under-the-radar insights

For each article, classify it into ONE category. Respond in this exact format:

GOOD NEWS:
[article numbers, e.g., 1, 3, 5]

BAD NEWS:
[article numbers, e.g., 2, 4]

HIDDEN EDGE:
[article numbers, e.g., 6, 7]

Only list article numbers. Ignore articles that don't fit any category clearly."""

    try:
        response = call_grok_api(prompt)
        if not response:
            return None
        
        # Parse response
        good_indices = []
        bad_indices = []
        hidden_indices = []
        
        lines = response.split('\n')
        current_section = None
        
        for line in lines:
            line_clean = line.strip()
            if 'GOOD NEWS' in line.upper():
                current_section = 'good'
            elif 'BAD NEWS' in line.upper():
                current_section = 'bad'
            elif 'HIDDEN EDGE' in line.upper():
                current_section = 'hidden'
            elif current_section and line_clean:
                # Extract numbers from the line
                import re
                numbers = re.findall(r'\d+', line_clean)
                if numbers:
                    indices = [int(n) - 1 for n in numbers if 0 < int(n) <= len(articles)]  # Convert to 0-indexed
                    if current_section == 'good':
                        good_indices.extend(indices)
                    elif current_section == 'bad':
                        bad_indices.extend(indices)
                    elif current_section == 'hidden':
                        hidden_indices.extend(indices)
        
        # Get actual articles
        good = [articles[i] for i in good_indices if i < len(articles)]
        bad = [articles[i] for i in bad_indices if i < len(articles)]
        hidden = [articles[i] for i in hidden_indices if i < len(articles)]
        
        logging.info(f"LLM classified {len(good)} good, {len(bad)} bad, {len(hidden)} hidden edge articles")
        
        return {
            'good': good,
            'bad': bad,
            'hidden': hidden
        }
        
    except Exception as e:
        logging.error(f"Error in LLM news classification: {e}")
        return None

def analyze_fundamentals_with_llm(ticker, news_articles, api_key=GROK_API_KEY):
    """
    Use Grok LLM to analyze ticker fundamentals based on news and provide investment grade assessment.
    """
    if not api_key or not news_articles:
        return {
            'fundamental_strength': 'Unknown',
            'investment_grade': 'N/A',
            'key_fundamentals': [],
            'risk_factors': [],
            'opportunities': [],
            'overall_assessment': 'Insufficient data for fundamental analysis'
        }
    
    try:
        # Format news for analysis
        news_text = "\n".join([
            f"â€¢ {article.get('title', 'No title')}: {article.get('description', 'No description')[:200]}..."
            for article in news_articles[:10]  # Limit to 10 most recent articles
        ])
        
        prompt = f"""You are a financial analyst specializing in fundamental analysis. Analyze the following news about {ticker} and provide a comprehensive fundamental assessment.

NEWS ARTICLES:
{news_text}

Please provide your analysis in the following JSON format:
{{
    "fundamental_strength": "Solid/Moderate/Weak",
    "investment_grade": "A+/A/A-/B+/B/B-/C+/C/C-/D",
    "key_fundamentals": [
        "Revenue growth trends",
        "Market position",
        "Competitive advantages",
        "Management quality",
        "Financial health"
    ],
    "risk_factors": [
        "Specific risks identified",
        "Market challenges",
        "Regulatory concerns",
        "Competitive threats"
    ],
    "opportunities": [
        "Growth opportunities",
        "Market expansion",
        "Product innovation",
        "Strategic advantages"
    ],
    "overall_assessment": "Comprehensive 2-3 sentence assessment of the company's fundamental strength and investment viability"
}}

Focus on:
1. Business model sustainability
2. Market position and competitive moats
3. Financial health indicators from news
4. Growth prospects and market opportunities
5. Management execution and strategic direction

Provide objective, data-driven analysis based on the news content."""

        response = call_grok_api(prompt, api_key)
        
        if response and response.strip():
            # Try to parse JSON response
            try:
                import json
                # Extract JSON from response (handle cases where LLM adds extra text)
                start_idx = response.find('{')
                end_idx = response.rfind('}') + 1
                
                if start_idx != -1 and end_idx != 0:
                    json_str = response[start_idx:end_idx]
                    analysis = json.loads(json_str)
                    
                    # Validate required fields
                    required_fields = ['fundamental_strength', 'investment_grade', 'key_fundamentals', 'risk_factors', 'opportunities', 'overall_assessment']
                    for field in required_fields:
                        if field not in analysis:
                            analysis[field] = 'Not available'
                    
                    return analysis
                else:
                    logging.warning("Could not extract JSON from Grok response")
            except json.JSONDecodeError as e:
                logging.warning(f"Failed to parse JSON from Grok response: {e}")
                logging.warning(f"Raw response: {response[:500]}...")
        
        # Fallback analysis if LLM fails
        return {
            'fundamental_strength': 'Moderate',
            'investment_grade': 'B',
            'key_fundamentals': ['Analysis based on limited news data'],
            'risk_factors': ['Limited information available'],
            'opportunities': ['Further analysis needed'],
            'overall_assessment': f'Limited fundamental analysis available for {ticker} based on current news.'
        }
        
    except Exception as e:
        logging.error(f"Error in fundamental analysis with LLM: {e}")
        return {
            'fundamental_strength': 'Unknown',
            'investment_grade': 'N/A',
            'key_fundamentals': ['Analysis failed'],
            'risk_factors': ['Technical error'],
            'opportunities': ['Unable to analyze'],
            'overall_assessment': f'Technical error in fundamental analysis for {ticker}'
        }

def analyze_ticker_with_llm(ticker, news_articles):
    """
    Use LLM to analyze ticker-specific news and provide comprehensive insights
    """
    if not news_articles or len(news_articles) == 0:
        return {
            'summary': f"No recent news found for {ticker}",
            'sentiment': 'neutral',
            'hype_detected': False,
            'hype_explanation': None,
            'key_insights': [],
            'risk_level': 'unknown'
        }
    
    news_text = format_news_for_prompt(news_articles)
    
    prompt = f"""Analyze these recent news articles about {ticker} and provide:

NEWS ARTICLES:
{news_text}

Please provide a structured analysis in the following format:

1. SUMMARY: 2-3 sentence summary of key developments
2. SENTIMENT: Overall sentiment (bullish/bearish/neutral) with confidence score
3. HYPE DETECTION: Is there promotional/pump-and-dump language? (yes/no) with explanation
4. KEY INSIGHTS: 3-5 bullet points of actionable insights
5. RISK LEVEL: Investment risk (low/medium/high/extreme) with reasoning

Be concise but thorough. Focus on facts and avoid speculation."""

    try:
        response = call_grok_api(prompt)
        if not response:
            return None
        
        # Parse LLM response
        analysis = {
            'summary': '',
            'sentiment': 'neutral',
            'hype_detected': False,
            'hype_explanation': None,
            'key_insights': [],
            'risk_level': 'medium',
            'raw_analysis': response
        }
        
        # Extract sections from response
        lines = response.split('\n')
        current_section = None
        
        for line in lines:
            line_lower = line.lower()
            
            if 'summary:' in line_lower:
                current_section = 'summary'
                analysis['summary'] = line.split(':', 1)[1].strip() if ':' in line else ''
            elif 'sentiment:' in line_lower:
                current_section = 'sentiment'
                sentiment_text = line.lower()
                if 'bullish' in sentiment_text:
                    analysis['sentiment'] = 'bullish'
                elif 'bearish' in sentiment_text:
                    analysis['sentiment'] = 'bearish'
                else:
                    analysis['sentiment'] = 'neutral'
            elif 'hype' in line_lower and 'detection' in line_lower:
                current_section = 'hype'
                if 'yes' in line.lower():
                    analysis['hype_detected'] = True
            elif 'risk' in line_lower and 'level' in line_lower:
                current_section = 'risk'
                risk_text = line.lower()
                if 'low' in risk_text:
                    analysis['risk_level'] = 'low'
                elif 'high' in risk_text or 'extreme' in risk_text:
                    analysis['risk_level'] = 'high'
                else:
                    analysis['risk_level'] = 'medium'
            elif line.strip().startswith('-') or line.strip().startswith('â€¢'):
                if current_section == 'key_insights' or 'insight' in lines[max(0, lines.index(line)-1)].lower():
                    analysis['key_insights'].append(line.strip())
            elif current_section == 'summary' and line.strip() and not ':' in line:
                analysis['summary'] += ' ' + line.strip()
            elif current_section == 'hype' and line.strip() and not ':' in line:
                if not analysis['hype_explanation']:
                    analysis['hype_explanation'] = line.strip()
        
        return analysis
        
    except Exception as e:
        logging.error(f"Error in LLM analysis: {e}")
        return None

def build_analysis_display(news_articles, llm_analysis, llm_sentiment, llm_insights, llm_risk,
                          summary, negative, hype, caveat, ticker, fundamental_analysis=None):
    """
    Helper function to build analysis, hype, and alarm text displays
    """
    # Ticker Analysis
    if len(news_articles) == 0:
        analysis = f"âŒ NO NEWS FOUND FOR {ticker}\n\n"
        analysis += "Possible reasons:\nâ€¢ NewsAPI key not configured\nâ€¢ Rate limit exceeded\nâ€¢ No news for this ticker\n"
        hype_text = "âš ï¸ Cannot detect hype without news data"
        alarm_text = "âš ï¸ Cannot check OTC status without data"
    elif llm_analysis and llm_analysis.get('raw_analysis'):
        # LLM-powered analysis
        sentiment_emoji = {'bullish': 'ðŸ“ˆ', 'bearish': 'ðŸ“‰', 'neutral': 'ðŸ“Š'}.get(llm_sentiment, 'ðŸ“Š')
        risk_emoji = {'low': 'ðŸŸ¢', 'medium': 'ðŸŸ¡', 'high': 'ðŸ”´', 'extreme': 'âš ï¸'}.get(llm_risk, 'ðŸŸ¡')
        
        analysis = f"ðŸ¤– AI-POWERED ANALYSIS ({len(news_articles)} articles)\n\n"
        analysis += f"ðŸ“° SUMMARY:\n{summary}\n\n"
        analysis += f"{sentiment_emoji} SENTIMENT: {llm_sentiment.upper()}\n\n"
        
        if llm_insights:
            analysis += f"ðŸ’¡ KEY INSIGHTS:\n"
            for insight in llm_insights[:5]:
                analysis += f"{insight}\n"
            analysis += "\n"
        
        analysis += f"{risk_emoji} RISK: {llm_risk.upper()}\n"
        
        # Add fundamental analysis if available
        if fundamental_analysis:
            strength_emoji = {'solid': 'ðŸŸ¢', 'moderate': 'ðŸŸ¡', 'weak': 'ðŸ”´'}.get(
                fundamental_analysis.get('fundamental_strength', '').lower(), 'ðŸŸ¡')
            grade = fundamental_analysis.get('investment_grade', 'N/A')
            overall = fundamental_analysis.get('overall_assessment', 'No assessment available')
            
            analysis += f"\n{strength_emoji} FUNDAMENTALS: {fundamental_analysis.get('fundamental_strength', 'Unknown').upper()}\n"
            analysis += f"ðŸ“Š INVESTMENT GRADE: {grade}\n\n"
            analysis += f"ðŸ“‹ OVERALL ASSESSMENT:\n{overall}\n\n"
            
            # Add key fundamentals
            key_fundamentals = fundamental_analysis.get('key_fundamentals', [])
            if key_fundamentals:
                analysis += "ðŸ”‘ KEY FUNDAMENTALS:\n"
                for fundamental in key_fundamentals[:3]:  # Show top 3
                    analysis += f"â€¢ {fundamental}\n"
                analysis += "\n"
            
            # Add opportunities
            opportunities = fundamental_analysis.get('opportunities', [])
            if opportunities:
                analysis += "ðŸš€ OPPORTUNITIES:\n"
                for opportunity in opportunities[:2]:  # Show top 2
                    analysis += f"â€¢ {opportunity}\n"
                analysis += "\n"
        
        # Hype detection
        if hype:
            hype_text = f"ðŸš¨ AI DETECTED HYPE!\n\n{hype}\n\nâš ï¸ WARNING: Possible promotion/manipulation."
        else:
            hype_text = "âœ“ No obvious hype detected."
    else:
        # Fallback analysis
        analysis = f"ðŸ“° NEWS SUMMARY ({len(news_articles)} articles):\n{summary}\n"
        if negative:
            analysis += f"\nðŸ“‰ SENTIMENT:\nPossible bearish: {negative}"
        else:
            analysis += "\nðŸ“Š SENTIMENT:\nNo strong negative detected."
        
        # Add fundamental analysis to fallback case too
        if fundamental_analysis:
            strength_emoji = {'solid': 'ðŸŸ¢', 'moderate': 'ðŸŸ¡', 'weak': 'ðŸ”´'}.get(
                fundamental_analysis.get('fundamental_strength', '').lower(), 'ðŸŸ¡')
            grade = fundamental_analysis.get('investment_grade', 'N/A')
            overall = fundamental_analysis.get('overall_assessment', 'No assessment available')
            
            analysis += f"\n{strength_emoji} FUNDAMENTALS: {fundamental_analysis.get('fundamental_strength', 'Unknown').upper()}\n"
            analysis += f"ðŸ“Š INVESTMENT GRADE: {grade}\n\n"
            analysis += f"ðŸ“‹ OVERALL ASSESSMENT:\n{overall}\n\n"
            
            # Add key fundamentals
            key_fundamentals = fundamental_analysis.get('key_fundamentals', [])
            if key_fundamentals:
                analysis += "ðŸ”‘ KEY FUNDAMENTALS:\n"
                for fundamental in key_fundamentals[:3]:  # Show top 3
                    analysis += f"â€¢ {fundamental}\n"
                analysis += "\n"
            
            # Add opportunities
            opportunities = fundamental_analysis.get('opportunities', [])
            if opportunities:
                analysis += "ðŸš€ OPPORTUNITIES:\n"
                for opportunity in opportunities[:2]:  # Show top 2
                    analysis += f"â€¢ {opportunity}\n"
                analysis += "\n"
        
        hype_text = f"âš ï¸ HYPE DETECTED!\n\n{hype}" if hype else "âœ“ No hype detected."
    
    # Caveat Emptor
    if caveat is True:
        alarm_text = f"ðŸš¨ CRITICAL WARNING!\n\nâš ï¸ Caveat Emptor detected!\n\nðŸ”— https://www.otcmarkets.com/stock/{ticker}/overview"
    elif isinstance(caveat, str) and caveat.startswith("Error"):
        alarm_text = f"âš ï¸ Could not verify OTC status:\n{caveat}"
    else:
        alarm_text = f"âœ“ No Caveat Emptor detected.\n\nâ„¹ï¸ OTCMarkets.com: Clean"
    
    return analysis, hype_text, alarm_text

# Update callback for ticker chart to also update analysis sections
@app.callback(
    [Output('ticker-chart-area', 'children'),
     Output('ticker-error', 'children'),
     Output('ticker-analysis-section', 'children'),
     Output('hype-section', 'children'),
     Output('alarm-section', 'children')],
    Input('generate-button', 'n_clicks'),
    State('ticker-input', 'value'),
    State('timeframe-dropdown', 'value')
)
def generate_ticker_charts(n_clicks, ticker, timeframe):
    if n_clicks == 0 or not ticker or not timeframe:
        return dash.no_update, "Please enter a ticker symbol and select a timeframe.", dash.no_update, dash.no_update, dash.no_update

    test_data = fetch_yfinance_data(ticker, period='5d', interval='1d')
    if test_data is None:
        return dash.no_update, f"Error: Unable to fetch data for ticker '{ticker}'.", dash.no_update, dash.no_update, dash.no_update

    timeframe_configs = {
        'daily': {'interval': '1d', 'period': '1y', 'label': 'Daily', 'default_days': 180},
        '1-minute': {'interval': '1m', 'period': '7d', 'label': '1-Minute', 'default_days': 7},
    }

    config = timeframe_configs.get(timeframe)
    if not config:
        return dash.no_update, "Invalid timeframe selected.", dash.no_update, dash.no_update, dash.no_update

    interval = config['interval']
    period = config['period']
    label = config['label']
    default_days = config['default_days']

    try:
        stock = yf.Ticker(ticker)
        df = stock.history(period=period, interval=interval)
        if df.empty:
            return dash.no_update, f"No data available for {ticker} on {label} timeframe.", dash.no_update, dash.no_update, dash.no_update
        df.index = pd.to_datetime(df.index)
        if df.index.tz is not None:
            df.index = df.index.tz_localize(None)
        sector_ticker = stock.info.get('sector', 'SPY')
        sector_ticker_map = {
            'Technology': 'XLK', 'Financial Services': 'XLF', 'Consumer Cyclical': 'XLY',
            'Industrials': 'XLI', 'Utilities': 'XLU', 'Healthcare': 'XLV',
            'Communication Services': 'XLC', 'Consumer Defensive': 'XLP',
            'Basic Materials': 'XLB', 'Real Estate': 'XLRE', 'Energy': 'XLE'
        }
        sector_ticker = sector_ticker_map.get(sector_ticker, 'SPY')
        sector_df = yf.Ticker(sector_ticker).history(period=period, interval=interval)
        sector_df.index = pd.to_datetime(sector_df.index)
        if sector_df.index.tz is not None:
            sector_df.index = sector_df.index.tz_localize(None)
        sector_df = sector_df.reindex(df.index, method='ffill').bfill()
        df['Sector_Close'] = sector_df['Close']
        df['Order_Flow'] = 0  # Placeholder
        df = df[['Open', 'High', 'Low', 'Close', 'Volume', 'Sector_Close', 'Order_Flow']]

        end_date = df.index[-1]
        if timeframe == 'daily':
            start_date = end_date - pd.Timedelta(days=180)
            df = df.loc[start_date:end_date]
        else:
            df = df[df.index.date == df.index[-1].date()]
            df = df.between_time('09:30', '16:00')
            if df.empty:
                return dash.no_update, f"No data for {ticker} on the last trading day (9:30 AM - 4:00 PM).", dash.no_update, dash.no_update, dash.no_update

        if len(df) < 2:
            return dash.no_update, f"Insufficient data for {ticker} on {label} timeframe.", dash.no_update, dash.no_update, dash.no_update

        # Enhanced Chart with Volume Analysis and Institutional Flow Detection
        from plotly.subplots import make_subplots
        
        # Create single subplot chart (no volume section)
        fig = make_subplots(
            rows=1, cols=1,
            subplot_titles=(f'{ticker} {label} Price Chart',),
            specs=[[{"secondary_y": False}]]
        )
        
        # Add historical candlesticks (single trace to avoid duplication)
        historical_candlestick = go.Candlestick(
            x=df.index,
            open=df['Open'],
            high=df['High'],
            low=df['Low'],
            close=df['Close'],
            name='Historical Data',
            increasing_line_color='green',
            decreasing_line_color='red'
        )
        fig.add_trace(historical_candlestick, row=1, col=1)
        
        # REMOVED: Volume bars section (as requested by user)
        # Only keep market indicators on the main price chart
        
        # Update layout with cleaner design - SINGLE CHART ONLY
        fig.update_layout(
            title=dict(
                text=f"{ticker} {label} Chart - Institutional Flow Analysis",
                x=0.5,
                xanchor='center',
                font=dict(size=16, color='#fff'),
                y=0.98  # Position title at the very top
            ),
            template='none',
            plot_bgcolor='#222',
            paper_bgcolor='#222',
            font_color='#fff',
            height=600,  # Reduced height for single chart
            margin=dict(l=80, r=80, t=120, b=80),  # Reasonable margins
            showlegend=True,
            legend=dict(
                orientation="h",
                yanchor="top",
                y=0.90,  # Position legend well below title
                xanchor="center",
                x=0.5,  # Centered
                font=dict(size=8),  # Smaller font
                bgcolor='rgba(0,0,0,0.8)',
                bordercolor='#666',
                borderwidth=1,
                itemsizing='constant',
                itemwidth=30  # Minimum allowed value
            )
        )
        
        # Update axes for single chart only
        fig.update_xaxes(
            color='#fff', 
            showgrid=True, 
            gridcolor='#444',
            row=1, col=1
        )
        fig.update_yaxes(
            color='#fff', 
            title="Price (USD)", 
            showgrid=True,
            gridcolor='#444',
            row=1, col=1
        )

        # ============================================================================
        # FETCH NEWS AND RUN ANALYSIS FIRST (before predictions)
        # This ensures news analysis always runs even if predictions fail/return early
        # ============================================================================
        try:
            logging.info(f"Fetching news for ticker: {ticker}")
            logging.info(f"NEWS_API_KEY set: {bool(NEWS_API_KEY)}")
            
            news_articles = fetch_comprehensive_news(NEWS_API_KEY, ticker=ticker)
            logging.info(f"Found {len(news_articles)} news articles for {ticker}")
            
            if len(news_articles) == 0:
                logging.error(f"âš ï¸ NO NEWS FOUND for {ticker}! Check:")
                logging.error(f"  - NEWS_API_KEY in .env file")
                logging.error(f"  - NewsAPI quota/rate limits")
                logging.error(f"  - RSS feed availability")
            
            # Log article details for debugging
            for i, article in enumerate(news_articles[:3]):
                logging.info(f"Article {i+1}: {article.get('title', 'No title')[:100]} (Source: {article.get('source', 'Unknown')})")
            
            # Use LLM for intelligent analysis
            logging.info(f"Running LLM analysis for {ticker}...")
            llm_analysis = analyze_ticker_with_llm(ticker, news_articles)
            
            # Run fundamental analysis
            logging.info(f"Running fundamental analysis for {ticker}...")
            if GROK_API_KEY:
                logging.info("GROK_API_KEY found, proceeding with fundamental analysis")
                fundamental_analysis = analyze_fundamentals_with_llm(ticker, news_articles, GROK_API_KEY)
                logging.info(f"Fundamental analysis complete - Strength: {fundamental_analysis.get('fundamental_strength', 'Unknown')}, Grade: {fundamental_analysis.get('investment_grade', 'N/A')}")
            else:
                logging.warning("GROK_API_KEY not found in .env file, skipping fundamental analysis")
                fundamental_analysis = {
                    'fundamental_strength': 'Unknown',
                    'investment_grade': 'N/A',
                    'key_fundamentals': ['GROK_API_KEY not configured'],
                    'risk_factors': ['API key missing'],
                    'opportunities': ['Configure GROK_API_KEY in .env file'],
                    'overall_assessment': 'Fundamental analysis unavailable - GROK_API_KEY not configured in .env file'
                }
            
            if llm_analysis and llm_analysis.get('raw_analysis'):
                # Use LLM results
                summary = llm_analysis.get('summary', summarize_news(news_articles))
                hype = llm_analysis.get('hype_explanation') if llm_analysis.get('hype_detected') else None
                negative = None  # LLM provides better sentiment analysis
                llm_sentiment = llm_analysis.get('sentiment', 'neutral')
                llm_insights = llm_analysis.get('key_insights', [])
                llm_risk = llm_analysis.get('risk_level', 'medium')
                logging.info(f"LLM Analysis complete - Sentiment: {llm_sentiment}, Hype: {llm_analysis.get('hype_detected')}, Risk: {llm_risk}")
            else:
                # Fallback to basic analysis if LLM fails
                logging.warning(f"LLM analysis unavailable, using fallback methods")
                summary = summarize_news(news_articles)
                negative = detect_negative_sentiment(news_articles)
                hype = detect_hype(news_articles)
                llm_analysis = None
                llm_sentiment = None
                llm_insights = []
                llm_risk = None
            
            logging.info(f"News summary: {summary[:100] if summary else 'None'}...")
            logging.info(f"Hype detected: {hype[:50] if hype else 'None'}")
            
        except Exception as e:
            logging.error(f"Error in news analysis for {ticker}: {e}", exc_info=True)
            news_articles = [{'title': 'News analysis temporarily unavailable', 'url': '', 'description': '', 'source': 'System'}]
            summary = f"News analysis failed: {str(e)}"
            negative = None
            hype = None
            llm_analysis = None
            llm_sentiment = None
            llm_insights = []
            llm_risk = None
        
        # Check OTC Caveat Emptor
        try:
            logging.info(f"Checking OTC Caveat Emptor status for: {ticker}")
            caveat = check_otc_caveat_emptor(ticker)
            logging.info(f"OTC Caveat Emptor result: {caveat}")
        except Exception as e:
            logging.error(f"Error checking OTC caveat emptor for {ticker}: {e}", exc_info=True)
            caveat = f"Error checking OTC status: {str(e)}"

        # Generate predictions using enhanced feature engineering
        if timeframe == 'daily' and len(df) >= 60:
            try:
                # Enhanced ticker-specific feature engineering using advanced 40+ feature system
                try:
                    # Start with original DataFrame
                    df_processed = df.copy()
                    
                    # Ensure we have the required columns
                    required_cols = ['Close', 'Volume', 'High', 'Low', 'Open']
                    missing_cols = [col for col in required_cols if col not in df_processed.columns]
                    if missing_cols:
                        logging.error(f"Missing required columns: {missing_cols}")
                        return dcc.Graph(figure=fig, config={'displayModeBar': False}, style={'height': '100%', 'backgroundColor': '#222'}), f"Missing required columns for {ticker}: {missing_cols}", dash.no_update, dash.no_update, dash.no_update
                    
                    # === ADVANCED ENHANCED FEATURE ENGINEERING (40+ FEATURES) ===
                    logging.info(f"Computing advanced enhanced features for {ticker}")
                    
                    try:
                        # Use the sophisticated enhanced feature engineering system
                        logging.info(f"DataFrame shape before enhanced features: {df_processed.shape}")
                        logging.info(f"DataFrame columns before enhanced features: {list(df_processed.columns)}")
                        
                        df_processed, feature_names = enhanced_feature_engineering(df_processed)
                        logging.info(f"Enhanced features computed: {len(feature_names)} features available")
                        
                        # Log the feature names for debugging
                        logging.info(f"Available enhanced features: {feature_names[:10]}...")  # Show first 10 features
                        logging.info(f"DataFrame shape after enhanced features: {df_processed.shape}")
                        logging.info(f"DataFrame columns after enhanced features: {list(df_processed.columns)}")
                        
                    except Exception as enhanced_error:
                        logging.warning(f"Enhanced feature engineering failed: {enhanced_error}, falling back to basic features")
                        
                        # Fallback to basic features if enhanced features fail
                        df_processed['Price_Change'] = df_processed['Close'].pct_change()
                        df_processed['Log_Returns'] = np.log(df_processed['Close'] / df_processed['Close'].shift(1))
                        df_processed['Volatility'] = df_processed['Close'].rolling(window=10).std()
                        df_processed['SMA_20'] = df_processed['Close'].rolling(window=20).mean()
                        df_processed['RSI'] = compute_rsi(df_processed['Close'], 14)
                        
                        feature_names = ['Close', 'Volume', 'Price_Change', 'Volatility', 'SMA_20', 'RSI']
                    
                    # Add institutional flow analysis (keep this custom function as it's specific to SPS)
                    logging.info(f"Computing institutional flow analysis for {ticker}")
                    logging.info(f"DataFrame shape before institutional flow: {df_processed.shape}")
                    logging.info(f"DataFrame columns before institutional flow: {list(df_processed.columns)}")
                    
                    df_processed = compute_institutional_flow_analysis(df_processed)
                    
                    logging.info(f"DataFrame shape after institutional flow: {df_processed.shape}")
                    logging.info(f"DataFrame columns after institutional flow: {list(df_processed.columns)}")
                    
                    # Verify institutional flow columns exist
                    required_columns = ['Institutional_Score', 'Capitulation', 'Institutional_Buying', 'Institutional_Selling', 'High_Price_Impact', 'Low_Price_Impact']
                    missing_columns = [col for col in required_columns if col not in df_processed.columns]
                    if missing_columns:
                        logging.warning(f"Missing institutional flow columns for {ticker}: {missing_columns}")
                    else:
                        logging.info(f"All institutional flow columns present for {ticker}")
                    
                    # Add sector relative features if not present
                    if 'Sector_Close' not in df_processed.columns:
                        df_processed['Sector_Close'] = df_processed['Close']  # Use Close as fallback
                        df_processed['Sector_Volatility'] = df_processed['Sector_Close'].rolling(window=10).std()
                    
                    # Ensure we have the essential features for the model
                    essential_features = ['Close', 'Volume', 'Price_Change', 'Volatility']
                    for feature in essential_features:
                        if feature not in df_processed.columns:
                            if feature == 'Price_Change':
                                df_processed[feature] = df_processed['Close'].pct_change()
                            elif feature == 'Volatility':
                                df_processed[feature] = df_processed['Close'].rolling(window=10).std()
                    
                    # Add Realized_Vol and Vol_Ratio for backward compatibility
                    if 'Realized_Vol' not in df_processed.columns:
                        df_processed['Realized_Vol'] = df_processed.get('Volatility', df_processed['Close'].rolling(window=10).std())
                    if 'Vol_Ratio' not in df_processed.columns:
                        vol_series = df_processed.get('Volatility', df_processed['Close'].rolling(window=10).std())
                        df_processed['Vol_Ratio'] = vol_series / vol_series.rolling(window=20).mean()
                    
                    # Add Order Flow for backward compatibility
                    if 'Order_Flow' not in df_processed.columns:
                        df_processed['Order_Flow'] = df_processed['Volume'] * df_processed.get('Price_Change', df_processed['Close'].pct_change())
                    
                    logging.info(f"Advanced enhanced features computed for {ticker}. DataFrame shape: {df_processed.shape}")
                    logging.info(f"Total features available: {len(df_processed.columns)}")
                    
                except Exception as e:
                    logging.error(f"Error computing enhanced features: {e}")
                    return dcc.Graph(figure=fig, config={'displayModeBar': False}, style={'height': '100%', 'backgroundColor': '#222'}), f"Error computing features for {ticker}: {str(e)}", dash.no_update, dash.no_update, dash.no_update
                
                # === ENHANCE CHART WITH INSTITUTIONAL FLOW ANALYSIS ===
                try:
                    logging.info(f"Starting chart enhancement for {ticker}")
                    enhance_chart_with_institutional_flow(fig, df_processed)
                    logging.info(f"Chart enhancement completed for {ticker}")
                except Exception as e:
                    logging.error(f"Error enhancing chart for {ticker}: {e}")
                    # Continue without enhancement if there's an error
                
                logging.info(f"Chart figure created with {len(fig.data)} traces")
                
                # Use the exact 14 features that the daily model was trained on for compatibility
                # This ensures the model input size matches what it expects
                daily_features = [
                    'Close', 'Volume', 'Price_Change', 'Volatility', 'Sector_Volatility', 
                    'Realized_Vol', 'Vol_Ratio', 'SMA_20', 'RSI', 'MACD', 
                    'Upper_BB', 'Lower_BB', 'ATR', 'Order_Flow'
                ]
                
                # Check which of these features are available in our enhanced DataFrame
                available_daily_features = [f for f in daily_features if f in df_processed.columns]
                missing_features = [f for f in daily_features if f not in df_processed.columns]
                
                logging.info(f'Model expects 14 features: {len(daily_features)}')
                logging.info(f'Available features: {len(available_daily_features)}')
                logging.info(f'Missing features: {missing_features}')
                
                # Create missing features if needed
                for feature in missing_features:
                    if feature == 'SMA_20' and 'SMA_20' not in df_processed.columns:
                        df_processed['SMA_20'] = df_processed['Close'].rolling(window=20).mean()
                    elif feature == 'RSI' and 'RSI' not in df_processed.columns:
                        df_processed['RSI'] = compute_rsi(df_processed['Close'], 14)
                    elif feature == 'MACD' and 'MACD' not in df_processed.columns:
                        ema_12 = df_processed['Close'].ewm(span=12, adjust=False).mean()
                        ema_26 = df_processed['Close'].ewm(span=26, adjust=False).mean()
                        df_processed['MACD'] = ema_12 - ema_26
                    elif feature == 'Upper_BB' and 'Upper_BB' not in df_processed.columns:
                        sma_bb = df_processed['Close'].rolling(window=20).mean()
                        std_bb = df_processed['Close'].rolling(window=20).std()
                        df_processed['Upper_BB'] = sma_bb + (std_bb * 2)
                    elif feature == 'Lower_BB' and 'Lower_BB' not in df_processed.columns:
                        sma_bb = df_processed['Close'].rolling(window=20).mean()
                        std_bb = df_processed['Close'].rolling(window=20).std()
                        df_processed['Lower_BB'] = sma_bb - (std_bb * 2)
                    elif feature == 'ATR' and 'ATR' not in df_processed.columns:
                        df_processed['ATR'] = (df_processed['High'] - df_processed['Low']).rolling(window=14).mean()
                
                # Final check - use exactly the 14 features the model expects
                daily_features = [f for f in daily_features if f in df_processed.columns]
                logging.info(f'Final features for model: {len(daily_features)} - {daily_features}')
                
                # Ensure we have exactly 14 features
                if len(daily_features) != 14:
                    logging.warning(f'Warning: Model expects 14 features but got {len(daily_features)}')
                    # Pad with zeros or use the most important features
                    if len(daily_features) < 14:
                        logging.error(f'Not enough features for model prediction: {len(daily_features)}/14')
                        return dcc.Graph(figure=fig, config={'displayModeBar': False}, style={'height': '100%', 'backgroundColor': '#222'}), f"Not enough features for {ticker} prediction: {len(daily_features)}/14 required", dash.no_update, dash.no_update, dash.no_update
                
                if df_processed.shape[0] < 60:
                    return dcc.Graph(figure=fig, config={'displayModeBar': False}, style={'height': '100%', 'backgroundColor': '#222'}), f"Not enough processed data for {ticker} to make daily predictions. Need at least 60 rows, got {df_processed.shape[0]}.", dash.no_update, dash.no_update, dash.no_update
                
                # === ENSEMBLE PREDICTION WITH CONFIDENCE SCORING ===
                
                # Use daily model features for prediction (matching trained model)
                logging.info(f"Creating sequence for prediction with features: {daily_features}")
                logging.info(f"Feature count: {len(daily_features)}")
                logging.info(f"DataFrame columns available: {list(df_processed.columns)}")
                
                # Ensure we have exactly 14 features as expected by the model
                if len(daily_features) != 14:
                    logging.error(f"CRITICAL: Model expects 14 features but got {len(daily_features)}")
                    logging.error(f"Features: {daily_features}")
                    return dcc.Graph(figure=fig, config={'displayModeBar': False}, style={'height': '100%', 'backgroundColor': '#222'}), f"Model input size error: Expected 14 features, got {len(daily_features)}", dash.no_update, dash.no_update, dash.no_update
                
                seq = df_processed.iloc[-60:][daily_features].values
                logging.info(f"Sequence shape: {seq.shape}")
                logging.info(f"Sequence contains NaN: {np.isnan(seq).any()}")
                logging.info(f"Sequence contains Inf: {np.isinf(seq).any()}")
                
                if seq.shape[0] < 1:
                    return dcc.Graph(figure=fig, config={'displayModeBar': False}, style={'height': '100%', 'backgroundColor': '#222'}), f"No valid data for {ticker} after feature engineering.", dash.no_update, dash.no_update, dash.no_update
                
                if seq.shape[1] != 14:
                    logging.error(f"CRITICAL: Sequence has {seq.shape[1]} features but model expects 14")
                    return dcc.Graph(figure=fig, config={'displayModeBar': False}, style={'height': '100%', 'backgroundColor': '#222'}), f"Sequence feature count error: Expected 14, got {seq.shape[1]}", dash.no_update, dash.no_update, dash.no_update
                
                # Handle NaN values in input sequence
                if np.isnan(seq).any():
                    logging.warning(f"NaN values found in input sequence: {np.isnan(seq).sum()} out of {seq.size}")
                    logging.info(f"Filling NaN values with forward fill and backward fill...")
                    
                    # Create DataFrame to handle NaN values properly
                    seq_df = pd.DataFrame(seq, columns=daily_features)
                    seq_df = seq_df.fillna(method='ffill').fillna(method='bfill')
                    
                    # If still NaN, fill with 0
                    seq_df = seq_df.fillna(0)
                    seq = seq_df.values
                    
                    logging.info(f"After NaN handling - Sequence contains NaN: {np.isnan(seq).any()}")
                    logging.info(f"Sequence min/max: {seq.min():.6f} / {seq.max():.6f}")
                
                # Scale features
                feature_scaler = MinMaxScaler()
                feature_scaler.fit(df_processed[daily_features])
                seq_scaled = feature_scaler.transform(seq)
                
                # Scale targets
                target_scaler = MinMaxScaler()
                close_values = df_processed['Close'].values.reshape(-1, 1)
                target_scaler.fit(close_values)
                
                # === MULTIPLE PREDICTION METHODS ===
                predictions_list = []
                confidence_scores = []
                
                # 1. Original LSTM Model Prediction
                seq_tensor = torch.tensor(seq_scaled, dtype=torch.float32).unsqueeze(0)
                logging.info(f"Tensor shape before model: {seq_tensor.shape}")
                logging.info(f"Model input_size: 14 (expected)")
                logging.info(f"Tensor contains NaN: {torch.isnan(seq_tensor).any()}")
                logging.info(f"Tensor contains Inf: {torch.isinf(seq_tensor).any()}")
                logging.info(f"Tensor min/max: {seq_tensor.min():.6f} / {seq_tensor.max():.6f}")
                
                with torch.no_grad():
                    lstm_pred = daily_model(seq_tensor).numpy().flatten()
                    logging.info(f"LSTM raw output shape: {lstm_pred.shape}")
                    logging.info(f"LSTM raw output contains NaN: {np.isnan(lstm_pred).any()}")
                    logging.info(f"LSTM raw output min/max: {np.nanmin(lstm_pred):.6f} / {np.nanmax(lstm_pred):.6f}")
                    logging.info(f"LSTM raw output first 5 values: {lstm_pred[:5]}")
                
                # Check if target_scaler has valid data
                logging.info(f"Target scaler data info:")
                logging.info(f"  Data shape: {close_values.shape}")
                logging.info(f"  Data contains NaN: {np.isnan(close_values).any()}")
                logging.info(f"  Data min/max: {np.nanmin(close_values):.6f} / {np.nanmax(close_values):.6f}")
                
                try:
                    lstm_pred_actual = target_scaler.inverse_transform(lstm_pred.reshape(-1, 1)).flatten()
                    logging.info(f"LSTM inverse transform successful")
                    logging.info(f"LSTM actual predictions contains NaN: {np.isnan(lstm_pred_actual).any()}")
                    logging.info(f"LSTM actual predictions min/max: {np.nanmin(lstm_pred_actual):.6f} / {np.nanmax(lstm_pred_actual):.6f}")
                    logging.info(f"LSTM actual predictions first 5: {lstm_pred_actual[:5]}")
                except Exception as transform_error:
                    logging.error(f"Error in inverse transform: {transform_error}")
                    # Fallback: use raw predictions without scaling
                    lstm_pred_actual = lstm_pred
                    logging.info(f"Using raw predictions as fallback")
                
                predictions_list.append(lstm_pred_actual)
                
                # Calculate confidence for LSTM prediction - IMPROVED
                recent_volatility = df_processed['Volatility'].iloc[-10:].mean()
                price_stability = 1 - (df_processed['Close'].iloc[-10:].std() / df_processed['Close'].iloc[-10:].mean())
                data_quality = min(1.0, len(df_processed) / 100)  # More data = higher confidence
                lstm_confidence = max(0.3, min(0.9, (1 - recent_volatility * 5) * price_stability * data_quality))
                confidence_scores.append(lstm_confidence)
                
                # 2. Trend-based Prediction (Simple Moving Average Extrapolation) - IMPROVED
                recent_trend = df_processed['Close'].iloc[-20:].pct_change().mean()
                last_price = df_processed['Close'].iloc[-1]
                trend_pred = []
                for i in range(30):
                    trend_pred.append(last_price * (1 + recent_trend) ** (i + 1))
                predictions_list.append(np.array(trend_pred))
                
                # Confidence for trend prediction - IMPROVED
                trend_consistency = 1 - abs(df_processed['Close'].iloc[-10:].pct_change().std())
                trend_strength = abs(recent_trend) * 100  # Stronger trends get higher confidence
                trend_confidence = max(0.4, min(0.9, trend_consistency * (0.5 + trend_strength)))
                confidence_scores.append(trend_confidence)
                
                # 3. Volatility-Adjusted Prediction - IMPROVED
                current_vol = df_processed['Volatility'].iloc[-1]
                avg_vol = df_processed['Volatility'].mean()
                vol_factor = current_vol / avg_vol if avg_vol > 0 else 1.0
                
                # Adjust LSTM predictions based on volatility - MORE REALISTIC
                vol_adjusted_pred = lstm_pred_actual.copy()
                if vol_factor > 1.2:  # High volatility
                    # Apply more conservative adjustments instead of random noise
                    vol_adjusted_pred = vol_adjusted_pred * (1 - (vol_factor - 1) * 0.1)
                elif vol_factor < 0.8:  # Low volatility
                    # Slightly amplify predictions in stable conditions
                    vol_adjusted_pred = vol_adjusted_pred * (1 + (0.8 - vol_factor) * 0.05)
                predictions_list.append(vol_adjusted_pred)
                
                # Confidence for volatility-adjusted prediction - IMPROVED
                vol_stability = 1 / (1 + abs(vol_factor - 1))  # Closer to 1.0 = more stable
                vol_confidence = max(0.3, min(0.8, vol_stability))
                confidence_scores.append(vol_confidence)
                
                # === ENSEMBLE COMBINATION ===
                # Weight predictions by confidence scores
                total_confidence = sum(confidence_scores)
                weights = [conf / total_confidence for conf in confidence_scores]
                
                # Calculate ensemble prediction
                ensemble_pred = np.zeros(30)
                for i, pred in enumerate(predictions_list):
                    ensemble_pred += weights[i] * pred
                
                # Calculate ensemble confidence
                ensemble_confidence = np.mean(confidence_scores)
                
                # Apply confidence-based smoothing - IMPROVED
                if ensemble_confidence < 0.6:
                    # Low confidence - apply more smoothing
                    ensemble_pred = gaussian_filter1d(ensemble_pred, sigma=1.2)
                    logging.info(f"Applied smoothing due to low confidence ({ensemble_confidence:.3f}) for {ticker}")
                
                # Add realistic daily variation instead of heavy smoothing
                base_predictions = ensemble_pred.copy()
                
                # Add realistic daily price movements to make it less linear
                for i in range(1, len(base_predictions)):
                    # Calculate daily change with some randomness based on historical volatility
                    recent_volatility = df_processed['Close'].pct_change().iloc[-20:].std()
                    daily_change_pct = np.random.normal(0, recent_volatility * 0.8)
                    
                    # Add small momentum effect
                    if i > 0:
                        prev_change = (base_predictions[i-1] - base_predictions[0]) / base_predictions[0]
                        momentum_factor = 1 + prev_change * 0.05  # Small momentum effect
                        daily_change_pct *= momentum_factor
                    
                    # Apply the change
                    base_predictions[i] = base_predictions[i-1] * (1 + daily_change_pct)
                
                # Light smoothing only to remove extreme noise
                ensemble_pred = gaussian_filter1d(base_predictions, sigma=0.3)
                
                # Ensure predictions follow realistic price movements
                last_actual_price = df_processed['Close'].iloc[-1]
                price_change_limit = last_actual_price * 0.1  # Max 10% change from last price
                
                # Cap extreme predictions
                for i in range(len(ensemble_pred)):
                    if abs(ensemble_pred[i] - last_actual_price) > price_change_limit:
                        if ensemble_pred[i] > last_actual_price:
                            ensemble_pred[i] = last_actual_price + price_change_limit
                        else:
                            ensemble_pred[i] = last_actual_price - price_change_limit
                
                predictions = ensemble_pred
                
                logging.info(f"Ensemble prediction completed for {ticker}")
                logging.info(f"Confidence scores: LSTM={confidence_scores[0]:.3f}, Trend={confidence_scores[1]:.3f}, Vol={confidence_scores[2]:.3f}")
                logging.info(f"Ensemble confidence: {ensemble_confidence:.3f}")
                logging.info(f"Raw model output (daily): {predictions[:5]}")  # Show first 5 predictions
                logging.info(f"Last 5 closing prices for {ticker} (Daily): {df['Close'].iloc[-5:].tolist()}")
                logging.info(f"Daily Predictions for {ticker} (first 5): {predictions[:5].tolist()}")
                
                # === IMPROVED MAE CALCULATION ===
                # Calculate MAE based on historical prediction accuracy
                historical_mae = 0
                if len(df_processed) > 80:  # Need enough data for validation
                    # Use last 20 days for validation
                    val_data = df_processed.iloc[-80:-20]  # Training data
                    val_target = df_processed.iloc[-20:]   # Validation target
                    
                    if len(val_data) >= 60:
                        # Make predictions on validation data
                        val_seq = val_data[daily_features].values[-60:]
                        val_seq_scaled = feature_scaler.transform(val_seq)
                        val_seq_tensor = torch.tensor(val_seq_scaled, dtype=torch.float32).unsqueeze(0)
                        
                        with torch.no_grad():
                            val_pred = daily_model(val_seq_tensor).numpy().flatten()
                        val_pred_actual = target_scaler.inverse_transform(val_pred.reshape(-1, 1)).flatten()
                        
                        # Calculate MAE for validation period
                        # The model predicts 30 days, but we only have 20 validation days
                        min_len = min(len(val_target['Close'].values), len(val_pred_actual))
                        if min_len > 0:
                            actual_prices = val_target['Close'].values[:min_len]
                            pred_prices = val_pred_actual[:min_len]
                            historical_mae = np.mean(np.abs(actual_prices - pred_prices))
                        else:
                            historical_mae = 5.0  # Default MAE
                    else:
                        historical_mae = 8.0  # Default for insufficient data
                else:
                    historical_mae = 10.0  # Default for insufficient data
                
                # Adjust MAE based on ensemble confidence
                confidence_adjusted_mae = historical_mae * (1 - ensemble_confidence * 0.5)
                final_mae = max(2.0, confidence_adjusted_mae)  # Minimum MAE of 2.0
                
                logging.info(f"Historical MAE: {historical_mae:.3f}, Ensemble Confidence: {ensemble_confidence:.3f}")
                logging.info(f"Final Confidence-Adjusted MAE: {final_mae:.3f}")
                
                last_date = df.index[-1]
                logging.info(f"Last historical date: {last_date}")
                future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=30, freq='B')
                logging.info(f"Generated future dates: {len(future_dates)} dates from {future_dates[0]} to {future_dates[-1]}")
                
                # Validate that we have valid predictions
                if len(predictions) != 30:
                    logging.error(f"CRITICAL: Expected 30 predictions but got {len(predictions)}")
                    return dcc.Graph(figure=fig, config={'displayModeBar': False}, style={'height': '100%', 'backgroundColor': '#222'}), f"Prediction count error: Expected 30, got {len(predictions)}", dash.no_update, dash.no_update, dash.no_update
                
                # Check for NaN values in predictions and provide fallback
                if np.isnan(predictions).any():
                    logging.error(f"CRITICAL: NaN values found in predictions: {np.isnan(predictions).sum()} out of {len(predictions)}")
                    logging.info(f"Generating fallback predictions based on recent price trends...")
                    
                    # Generate simple fallback predictions based on recent trend
                    last_price = df['Close'].iloc[-1]
                    recent_trend = df['Close'].iloc[-10:].pct_change().mean()
                    recent_volatility = df['Close'].iloc[-10:].std()
                    
                    # Create simple trend-based predictions
                    fallback_predictions = []
                    for i in range(30):
                        # Simple linear trend with some volatility
                        trend_factor = 1 + (recent_trend * (i + 1))
                        volatility_factor = 1 + (np.random.normal(0, recent_volatility * 0.1))
                        predicted_price = last_price * trend_factor * volatility_factor
                        fallback_predictions.append(predicted_price)
                    
                    predictions = np.array(fallback_predictions)
                    logging.info(f"Fallback predictions generated: {len(predictions)} values, range: {predictions.min():.2f} to {predictions.max():.2f}")
                    logging.info(f"Fallback predictions first 5: {predictions[:5]}")
                
                # === IMPROVED OHLC GENERATION ===
                pred_open = [df['Close'].iloc[-1]] + list(predictions[:-1])
                pred_close = predictions
                
                # Enhanced OHLC calculation for more realistic candlesticks
                confidence_vol = df_processed['Volatility'].iloc[-1] * (1 - ensemble_confidence * 0.2)
                base_price = np.array(predictions)
                
                # Calculate realistic daily ranges based on historical patterns
                historical_ranges = (df_processed['High'] - df_processed['Low']).iloc[-20:].mean()
                daily_range_ratio = historical_ranges / df_processed['Close'].iloc[-1]
                
                # Generate realistic high and low for each day with more volatility
                pred_high = []
                pred_low = []
                
                # Calculate realistic daily volatility based on recent patterns
                recent_daily_ranges = (df_processed['High'] - df_processed['Low']) / df_processed['Close']
                avg_daily_range_pct = recent_daily_ranges.iloc[-20:].mean()
                
                for i in range(len(predictions)):
                    # Add realistic daily volatility to predictions
                    daily_volatility = predictions[i] * avg_daily_range_pct * np.random.uniform(0.8, 1.5)
                    
                    # Add some trend-based movement (not just straight line)
                    if i > 0:
                        # Add momentum from previous day with some randomness
                        momentum = (predictions[i] - predictions[i-1]) * np.random.uniform(0.5, 1.2)
                        daily_volatility += abs(momentum) * 0.3
                    
                    # Generate high and low with realistic ranges
                    high_addition = daily_volatility * np.random.uniform(0.3, 0.6)
                    low_subtraction = daily_volatility * np.random.uniform(0.3, 0.6)
                    
                    pred_high.append(predictions[i] + high_addition)
                    pred_low.append(predictions[i] - low_subtraction)
                
                pred_high = np.array(pred_high)
                pred_low = np.array(pred_low)
                
                # Ensure OHLC relationships are maintained
                pred_high = np.maximum(pred_high, np.maximum(pred_open, pred_close))
                pred_low = np.minimum(pred_low, np.minimum(pred_open, pred_close))
                
                # Debug prediction data
                logging.info(f"Prediction data generated:")
                logging.info(f"Future dates: {len(future_dates)} dates from {future_dates[0]} to {future_dates[-1]}")
                logging.info(f"Prediction prices: {len(predictions)} values, range: {predictions.min():.2f} to {predictions.max():.2f}")
                logging.info(f"Pred Open: {len(pred_open)} values, range: {min(pred_open):.2f} to {max(pred_open):.2f}")
                logging.info(f"Pred Close: {len(pred_close)} values, range: {min(pred_close):.2f} to {max(pred_close):.2f}")
                logging.info(f"Pred High: {len(pred_high)} values, range: {pred_high.min():.2f} to {pred_high.max():.2f}")
                logging.info(f"Pred Low: {len(pred_low)} values, range: {pred_low.min():.2f} to {pred_low.max():.2f}")
                pred_df = pd.DataFrame({
                    'Date': future_dates,
                    'Predicted_Open': pred_open,
                    'Predicted_High': pred_high,
                    'Predicted_Low': pred_low,
                    'Predicted_Close': pred_close,
                    'Actual_Close': [None] * len(future_dates)
                })
                excel_filename = os.path.join(r"C:\Users\jonel\OneDrive\Desktop\Jonel_Projects\Market_Analysis\funda\data", f"predictions_{ticker}_daily.xlsx")
                pred_df.to_excel(excel_filename, index=False)
                logging.info(f"Saved daily predictions for {ticker} to {excel_filename}")
                # Create prediction candlestick chart
                logging.info(f"Creating prediction candlestick chart...")
                pred_candlestick = go.Candlestick(
                    x=future_dates,
                    open=pred_open,
                    high=pred_high,
                    low=pred_low,
                    close=pred_close,
                    name='Predicted (30 Days)',
                    increasing_line_color='#39FF14',  # Neon green for up days
                    decreasing_line_color='#FF6B6B',  # Light red for down days
                    increasing_fillcolor='rgba(57, 255, 20, 0.3)',  # Semi-transparent green
                    decreasing_fillcolor='rgba(255, 107, 107, 0.3)',  # Semi-transparent red
                    hoverinfo='x+y',
                    hoverlabel=dict(
                        bgcolor='rgba(0,0,0,0.8)',
                        bordercolor='#39FF14',
                        font_color='white'
                    )
                )
                
                logging.info(f"Adding prediction candlestick trace to chart...")
                logging.info(f"Chart before adding predictions: {len(fig.data)} traces")
                
                # Add prediction candlesticks ONLY to price chart (row=1, col=1)
                fig.add_trace(pred_candlestick, row=1, col=1)
                logging.info(f"Added prediction candlesticks. Chart now has {len(fig.data)} traces")
                
                # Force update the x-axis range to include predictions
                logging.info(f"Updating x-axis range from {df.index[0]} to {future_dates[-1]}")
                
                # Update x-axis range for single chart
                fig.update_xaxes(range=[df.index[0], future_dates[-1]], row=1, col=1)
                
                # Also update layout to ensure predictions are visible
                fig.update_layout(
                    xaxis=dict(range=[df.index[0], future_dates[-1]]),
                    showlegend=True
                )
                logging.info(f"Updated chart layout and x-axis range")
                
                # Enhanced title with confidence metrics
                confidence_percentage = ensemble_confidence * 100
                title_text = f"{ticker} {label} Chart - MAE: {final_mae:.2f} | Confidence: {confidence_percentage:.1f}%"
                fig.update_layout(title=title_text)
                
                # Return the chart with daily predictions AND news analysis
                logging.info(f"Daily predictions completed for {ticker}, returning chart with analysis")
                
                # Build analysis text (moved here from later in the code)
                analysis_text, hype_text, alarm_text = build_analysis_display(
                    news_articles, llm_analysis, llm_sentiment, llm_insights, llm_risk,
                    summary, negative, hype, caveat, ticker, fundamental_analysis
                )
                
                return dcc.Graph(figure=fig, config={'displayModeBar': False}, style={'height': '100%', 'backgroundColor': '#222'}), f"Chart generated successfully", analysis_text, hype_text, alarm_text
                
            except Exception as e:
                logging.error(f"Error generating daily predictions for {ticker}: {e}")
                return dcc.Graph(figure=fig, config={'displayModeBar': False}, style={'height': '100%', 'backgroundColor': '#222'}), f"Chart generated, but error in daily predictions: {str(e)}.", dash.no_update, dash.no_update, dash.no_update
        elif timeframe == '1-minute' and len(df) >= 30:
            try:
                df['Price_Change'] = df['Close'].pct_change()
                df['Volatility'] = df['Close'].rolling(window=10).std()
                df['Sector_Volatility'] = df['Sector_Close'].rolling(window=10).std()
                df['Realized_Vol'] = compute_realized_volatility(df, window=10)
                df['Vol_Ratio'] = df['Volatility'] / df['Volatility'].rolling(window=20).mean()
                df['Momentum'] = df['Close'].diff(3)
                df['Volume_Spike'] = (df['Volume'] / df['Volume'].rolling(window=10).mean()) - 1
                features = ['Close', 'Volume', 'Price_Change', 'Volatility', 'Sector_Volatility', 'Realized_Vol', 'Vol_Ratio', 'Momentum', 'Volume_Spike']
                df_processed = df[features].dropna()
                logging.info(f'Rows after dropna (1-minute): {df_processed.shape[0]}')
                if df_processed.shape[0] < 30:
                    return dcc.Graph(figure=fig, config={'displayModeBar': False}, style={'height': '100%', 'backgroundColor': '#222'}), f"Not enough processed data for {ticker} to make 1-minute predictions. Need at least 30 rows, got {df_processed.shape[0]}.", dash.no_update, dash.no_update, dash.no_update
                order_footprint = fetch_order_footprint(ticker, period='7d', interval='1m', expected_rows=len(df_processed))
                order_footprint = order_footprint[-30:]
                seq = df_processed.iloc[-30:][features].values
                if seq.shape[0] < 1:
                    return dcc.Graph(figure=fig, config={'displayModeBar': False}, style={'height': '100%', 'backgroundColor': '#222'}), f"No valid data for {ticker} after feature engineering.", dash.no_update, dash.no_update, dash.no_update
                feature_scaler = MinMaxScaler()
                feature_scaler.fit(df_processed[features])
                seq_scaled = feature_scaler.transform(seq)
                target_scaler = MinMaxScaler()
                close_values = df_processed['Close'].values.reshape(-1, 1)
                target_scaler.fit(close_values)
                seq_tensor = torch.tensor(seq_scaled, dtype=torch.float32).unsqueeze(0)
                order_tensor = torch.tensor(order_footprint, dtype=torch.float32).unsqueeze(0)
                logging.info(f"Scaled input to model (1-minute): {seq_scaled}")
                with torch.no_grad():
                    predictions = minute_model(order_tensor, seq_tensor).numpy().flatten()
                logging.info(f"Raw model output (1-minute): {predictions}")
                predictions = target_scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()
                logging.info(f"Last 5 closing prices for {ticker} (1-Minute): {df['Close'].iloc[-5:].tolist()}")
                logging.info(f"1-Minute Predictions for {ticker} (first 5): {predictions[:5].tolist()}")
                last_date = df.index[-1]
                future_dates = pd.date_range(start=last_date + pd.Timedelta(minutes=1), periods=30, freq='1min')
                pred_open = [df['Close'].iloc[-1]] + list(predictions[:-1])
                pred_close = predictions
                pred_high = np.maximum(pred_open, pred_close) + predictions * 0.001
                pred_low = np.minimum(pred_open, pred_close) - predictions * 0.001
                pred_df = pd.DataFrame({
                    'Date': future_dates,
                    'Predicted_Open': pred_open,
                    'Predicted_High': pred_high,
                    'Predicted_Low': pred_low,
                    'Predicted_Close': pred_close,
                    'Actual_Close': [None] * len(future_dates)
                })
                excel_filename = os.path.join(r"C:\Users\jonel\OneDrive\Desktop\Jonel_Projects\Market_Analysis\funda\data", f"predictions_{ticker}_1minute.xlsx")
                pred_df.to_excel(excel_filename, index=False)
                logging.info(f"Saved 1-minute predictions for {ticker} to {excel_filename}")
                # Create prediction line chart (instead of candlesticks to avoid duplication)
                pred_line = go.Scatter(
                    x=future_dates,
                    y=pred_close,
                    mode='lines+markers',
                    name='Predicted (30 Minutes)',
                    line=dict(color='#39FF14', width=3),  # Neon green line
                    marker=dict(size=6, color='#39FF14', symbol='circle'),
                    hovertemplate='<b>Predicted Price</b><br>' +
                                 'Date: %{x}<br>' +
                                 'Price: $%{y:.2f}<br>' +
                                 '<extra></extra>'
                )
                fig.add_trace(pred_line, row=1, col=1)
                fig.update_xaxes(range=[df.index[0], future_dates[-1]])
                actual_close = df['Close'].iloc[-30:].values
                if len(actual_close) == len(predictions):
                    mae = mean_absolute_error(actual_close, predictions[:len(actual_close)])
                    fig.add_annotation(text=f"MAE: {mae:.4f}", xref="paper", yref="paper", x=0.5, y=0.95, showarrow=False)
                
                # Return the chart with 1-minute predictions - IMPORTANT: prevents further execution
                logging.info(f"1-minute predictions completed for {ticker}, returning chart")
                return dcc.Graph(figure=fig, config={'displayModeBar': False}, style={'height': '100%', 'backgroundColor': '#222'}), f"1-minute chart generated successfully", dash.no_update, dash.no_update, dash.no_update
                
            except Exception as e:
                logging.error(f"Error generating 1-minute predictions for {ticker}: {e}")
                return dcc.Graph(figure=fig, config={'displayModeBar': False}, style={'height': '100%', 'backgroundColor': '#222'}), f"Chart generated, but error in 1-minute predictions: {str(e)}.", dash.no_update, dash.no_update, dash.no_update
        elif timeframe == '1-minute' and len(df) < 30:
            return dcc.Graph(figure=fig, config={'displayModeBar': False}, style={'height': '100%', 'backgroundColor': '#222'}), f"Chart generated, but insufficient data for 1-minute predictions: {len(df)} rows.", dash.no_update, dash.no_update, dash.no_update

        # After chart code, fetch news and run LLM-powered analysis
        try:
            logging.info(f"Fetching news for ticker: {ticker}")
            logging.info(f"NEWS_API_KEY set: {bool(NEWS_API_KEY)}")
            
            news_articles = fetch_comprehensive_news(NEWS_API_KEY, ticker=ticker)
            logging.info(f"Found {len(news_articles)} news articles for {ticker}")
            
            if len(news_articles) == 0:
                logging.warning(f"âš ï¸ NO NEWS FOUND for {ticker}! This could be due to:")
                logging.warning(f"  - NewsAPI authentication issues (401 error)")
                logging.warning(f"  - NewsAPI quota exceeded")
                logging.warning(f"  - RSS feed availability issues")
                logging.warning(f"  - No recent news for this ticker")
                logging.warning(f"Chart will still be generated without news analysis")
            
            # Log article details for debugging
            for i, article in enumerate(news_articles[:3]):
                logging.info(f"Article {i+1}: {article.get('title', 'No title')[:100]} (Source: {article.get('source', 'Unknown')})")
            
            # Use LLM for intelligent analysis
            logging.info(f"Running LLM analysis for {ticker}...")
            llm_analysis = analyze_ticker_with_llm(ticker, news_articles)
            
            # Run fundamental analysis
            logging.info(f"Running fundamental analysis for {ticker}...")
            if GROK_API_KEY:
                logging.info("GROK_API_KEY found, proceeding with fundamental analysis")
                fundamental_analysis = analyze_fundamentals_with_llm(ticker, news_articles, GROK_API_KEY)
                logging.info(f"Fundamental analysis complete - Strength: {fundamental_analysis.get('fundamental_strength', 'Unknown')}, Grade: {fundamental_analysis.get('investment_grade', 'N/A')}")
            else:
                logging.warning("GROK_API_KEY not found in .env file, skipping fundamental analysis")
                fundamental_analysis = {
                    'fundamental_strength': 'Unknown',
                    'investment_grade': 'N/A',
                    'key_fundamentals': ['GROK_API_KEY not configured'],
                    'risk_factors': ['API key missing'],
                    'opportunities': ['Configure GROK_API_KEY in .env file'],
                    'overall_assessment': 'Fundamental analysis unavailable - GROK_API_KEY not configured in .env file'
                }
            
            if llm_analysis and llm_analysis.get('raw_analysis'):
                # Use LLM results
                summary = llm_analysis.get('summary', summarize_news(news_articles))
                hype = llm_analysis.get('hype_explanation') if llm_analysis.get('hype_detected') else None
                negative = None  # LLM provides better sentiment analysis
                llm_sentiment = llm_analysis.get('sentiment', 'neutral')
                llm_insights = llm_analysis.get('key_insights', [])
                llm_risk = llm_analysis.get('risk_level', 'medium')
                logging.info(f"LLM Analysis complete - Sentiment: {llm_sentiment}, Hype: {llm_analysis.get('hype_detected')}, Risk: {llm_risk}")
            else:
                # Fallback to basic analysis if LLM fails
                logging.warning(f"LLM analysis unavailable, using fallback methods")
                summary = summarize_news(news_articles)
                negative = detect_negative_sentiment(news_articles)
                hype = detect_hype(news_articles)
                llm_analysis = None
                llm_sentiment = None
                llm_insights = []
                llm_risk = None
            
            logging.info(f"News summary: {summary[:100] if summary else 'None'}...")
            logging.info(f"Hype detected: {hype[:50] if hype else 'None'}")
            
        except Exception as e:
            logging.error(f"Error in news analysis for {ticker}: {e}", exc_info=True)
            news_articles = [{'title': 'News analysis temporarily unavailable', 'url': '', 'description': '', 'source': 'System'}]
            summary = f"News analysis failed: {str(e)}"
            negative = None
            hype = None
            llm_analysis = None
            llm_sentiment = None
            llm_insights = []
            llm_risk = None
        
        try:
            logging.info(f"Checking OTC Caveat Emptor status for: {ticker}")
            caveat = check_otc_caveat_emptor(ticker)
            logging.info(f"OTC Caveat Emptor result: {caveat}")
        except Exception as e:
            logging.error(f"Error checking OTC caveat emptor for {ticker}: {e}", exc_info=True)
            caveat = f"Error checking OTC status: {str(e)}"

        # Volume Analysis Summary (if available)
        volume_analysis_summary = ""
        try:
            if 'df_processed' in locals() and hasattr(df_processed, 'Institutional_Score'):
                volume_analysis_summary = f"\n\n{generate_volume_analysis_summary(df_processed)}"
        except Exception as e:
            logging.error(f"Error generating volume analysis summary: {e}")
            volume_analysis_summary = "\n\nðŸ“Š VOLUME ANALYSIS: Error generating analysis"

        # Ticker Analysis - LLM-Powered or Fallback
        if len(news_articles) == 0:
            # No news found - show helpful message
            analysis = f"âŒ NO NEWS FOUND FOR {ticker}\n\n"
            analysis += "Possible reasons:\n"
            analysis += "â€¢ NewsAPI key not configured (check .env file)\n"
            analysis += "â€¢ NewsAPI rate limit exceeded\n"
            analysis += "â€¢ Ticker symbol not in news sources\n"
            analysis += "â€¢ RSS feeds unavailable\n\n"
            analysis += "ðŸ’¡ TO FIX:\n"
            analysis += "1. Add NEWS_API_KEY to your .env file\n"
            analysis += "2. Get free key from: https://newsapi.org\n"
            analysis += "3. Restart the app\n"
            
            hype_text = "âš ï¸ Cannot detect hype without news data"
            alarm = "âš ï¸ Cannot check OTC status - checking manually..."
            
        elif llm_analysis and llm_analysis.get('raw_analysis'):
            # Use LLM analysis for rich insights
            sentiment_emoji = {
                'bullish': 'ðŸ“ˆ',
                'bearish': 'ðŸ“‰',
                'neutral': 'ðŸ“Š'
            }.get(llm_sentiment, 'ðŸ“Š')
            
            risk_emoji = {
                'low': 'ðŸŸ¢',
                'medium': 'ðŸŸ¡',
                'high': 'ðŸ”´',
                'extreme': 'âš ï¸'
            }.get(llm_risk, 'ðŸŸ¡')
            
            analysis = f"ðŸ¤– AI-POWERED ANALYSIS ({len(news_articles)} articles analyzed)\n\n"
            analysis += f"ðŸ“° SUMMARY:\n{summary}\n\n"
            analysis += f"{sentiment_emoji} MARKET SENTIMENT: {llm_sentiment.upper()}\n\n"
            
            if llm_insights:
                analysis += f"ðŸ’¡ KEY INSIGHTS:\n"
                for insight in llm_insights[:5]:
                    analysis += f"{insight}\n"
                analysis += "\n"
            
            analysis += f"{risk_emoji} RISK LEVEL: {llm_risk.upper()}\n"
            
            # Add volume analysis if available
            analysis += volume_analysis_summary
            
            # Add news source breakdown
            sources = {}
            for article in news_articles[:10]:
                source = article.get('source', 'Unknown')
                sources[source] = sources.get(source, 0) + 1
            
            if sources:
                source_info = "\n\nðŸ“¡ NEWS SOURCES:\n" + "\n".join([f"- {src}: {count} article(s)" for src, count in sources.items()])
                analysis += source_info
        else:
            # Fallback to basic analysis
            analysis = f"ðŸ“° NEWS SUMMARY ({len(news_articles)} articles):\n{summary}\n"
            
        if negative:
                analysis += f"\nðŸ“‰ SENTIMENT ANALYSIS:\nPossible reason for being down: {negative}"
        else:
                analysis += "\nðŸ“Š SENTIMENT ANALYSIS:\nNo strong negative sentiment detected."
        
        # Add volume analysis to the main analysis
        analysis += volume_analysis_summary

            # Add news source breakdown
        sources = {}
        for article in news_articles[:10]:
                source = article.get('source', 'Unknown')
                sources[source] = sources.get(source, 0) + 1
            
        if sources:
                source_info = "\n\nðŸ“¡ NEWS SOURCES:\n" + "\n".join([f"- {src}: {count} article(s)" for src, count in sources.items()])
                analysis += source_info

        # Hype/Promotion - LLM or keyword-based
        if hype:
            if llm_analysis and llm_analysis.get('hype_detected'):
                hype_text = f"ðŸš¨ AI DETECTED HYPE/PROMOTION!\n\n{hype}\n\nâš ï¸ WARNING: This ticker may be subject to promotion or manipulation. Exercise extreme caution and do thorough due diligence."
            else:
                hype_text = f"âš ï¸ HYPE/PROMOTION DETECTED!\n\nðŸ“¢ {hype}\n\nðŸš¨ Warning: This ticker may be subject to promotion or pump-and-dump schemes. Exercise caution and do your own research."
        else:
            hype_text = "âœ“ No obvious hype or promotional language detected in recent news."

        # Caveat Emptor/Prohibited Alarm - Enhanced display
        if caveat is True:
            alarm = f"ðŸš¨ CRITICAL WARNING! ðŸš¨\n\nâš ï¸ Caveat Emptor or Prohibited status detected on OTCMarkets.com!\n\nâŒ This stock has serious regulatory concerns. Trading is extremely risky.\n\nðŸ”— Check details: https://www.otcmarkets.com/stock/{ticker}/overview"
        elif isinstance(caveat, str) and (caveat.startswith("Error") or caveat.startswith("OTC") or caveat.startswith("Network")):
            alarm = f"âš ï¸ Could not verify OTC status:\n{caveat}\n\nâ„¹ï¸ This doesn't necessarily indicate a problem, but verification failed."
        else:
            alarm = f"âœ“ No Caveat Emptor or Prohibited status detected.\n\nâ„¹ï¸ Checked: OTCMarkets.com (Status: Clean)"

        # Final debug - check chart traces before returning
        logging.info(f"FINAL CHART DEBUG:")
        logging.info(f"Total traces in chart: {len(fig.data)}")
        for i, trace in enumerate(fig.data):
            logging.info(f"Trace {i}: {trace.name} - Type: {type(trace).__name__}")
            if hasattr(trace, 'x') and trace.x is not None:
                logging.info(f"  X data length: {len(trace.x) if hasattr(trace.x, '__len__') else 'scalar'}")
            if hasattr(trace, 'y') and trace.y is not None:
                logging.info(f"  Y data length: {len(trace.y) if hasattr(trace.y, '__len__') else 'scalar'}")
        
        return dcc.Graph(
            figure=fig,
            config={'displayModeBar': True},
            style={
                'height': '100%',
                'backgroundColor': '#222'
            }
        ), "", analysis, hype_text, alarm

    except Exception as e:
        logging.error(f"Error generating chart for {ticker} ({label}): {e}")
        return dash.no_update, f"Error generating chart for {ticker} ({label}): {str(e)}.", dash.no_update, dash.no_update, dash.no_update

def fetch_economic_news(api_key, query="economy OR stock market OR inflation OR fed OR interest rates OR GDP OR unemployment OR earnings OR central bank OR monetary policy", language="en", page_size=15):
    """
    Fetch economic news from NewsAPI with expanded query terms and business filtering
    """
    if not api_key:
        logging.warning("NewsAPI key not provided, skipping NewsAPI request")
        return []
    
    # Add business domains filter to avoid sports/entertainment news
    domains = "bloomberg.com,reuters.com,cnbc.com,wsj.com,marketwatch.com,forbes.com,businessinsider.com,ft.com,seekingalpha.com,barrons.com,yahoo.com"
    
    url = (
        f"https://newsapi.org/v2/everything?"
        f"q={query}&language={language}&domains={domains}&sortBy=publishedAt&pageSize={page_size}&apiKey={api_key}"
    )
    
    try:
        logging.info(f"NewsAPI request: {url[:150]}...")
        response = requests.get(url, timeout=10)
        
        logging.info(f"NewsAPI response status: {response.status_code}")
        
        if response.status_code == 200:
            data = response.json()
            articles = data.get("articles", [])
            logging.info(f"NewsAPI returned {len(articles)} articles, total results: {data.get('totalResults', 0)}")
            
            # Additional filtering to remove sports/entertainment
            filtered_articles = []
            unwanted_keywords = ['football', 'soccer', 'nfl', 'nba', 'mlb', 'nhl', 'sports', 
                               'game', 'player', 'team', 'score', 'injury', 'wrestling', 
                               'celebrity', 'movie', 'music', 'entertainment']
            
            for article in articles:
                title_lower = (article.get('title') or '').lower()
                desc_lower = (article.get('description') or '').lower()
                content = title_lower + ' ' + desc_lower
                
                # Skip if contains unwanted keywords
                if any(word in content for word in unwanted_keywords):
                    logging.debug(f"Filtered out non-financial article: {title_lower[:50]}")
                    continue
                
                filtered_articles.append(article)
            
            logging.info(f"After filtering: {len(filtered_articles)} financial articles remain")
            return filtered_articles
            
        elif response.status_code == 401:
            logging.error("NewsAPI authentication failed (401). Check your API key and quota.")
            logging.error("This could be due to:")
            logging.error("  - Invalid API key")
            logging.error("  - API quota exceeded")
            logging.error("  - API key expired")
            return []
            
        elif response.status_code == 429:
            logging.error("NewsAPI rate limit exceeded (429). Too many requests.")
            return []
            
        else:
            try:
                error_data = response.json() if response.content else {}
                logging.error(f"NewsAPI error: {response.status_code} - {error_data}")
            except:
                logging.error(f"NewsAPI error: {response.status_code} - Could not parse error response")
            return []
            
    except requests.exceptions.Timeout:
        logging.error("NewsAPI request timed out after 10 seconds")
        return []
    except requests.exceptions.RequestException as e:
        logging.error(f"NewsAPI request failed: {e}")
        return []
    except Exception as e:
        logging.error(f"Unexpected error in NewsAPI request: {e}")
        return []

def fetch_yahoo_finance_news(ticker=None):
    """
    Fetch news from Yahoo Finance
    """
    try:
        if ticker:
            url = f"https://finance.yahoo.com/quote/{ticker}/news"
        else:
            url = "https://finance.yahoo.com/markets"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        articles = []
        # Look for news articles in Yahoo Finance format
        news_items = soup.find_all(['h3', 'h4'], class_=['Mb(5px)', 'Fz(16px)', 'Fw(b)'])
        
        for item in news_items[:10]:  # Limit to 10 articles
            link = item.find('a')
            if link:
                title = link.get_text(strip=True)
                href = link.get('href', '')
                if href.startswith('/'):
                    href = f"https://finance.yahoo.com{href}"
                
                articles.append({
                    'title': title,
                    'url': href,
                    'description': '',
                    'source': 'Yahoo Finance'
                })
        
        return articles
    except Exception as e:
        logging.error(f"Error fetching Yahoo Finance news: {e}")
        return []

def fetch_reuters_news(query="economy OR stock market OR inflation"):
    """
    Fetch news from Reuters RSS feed
    """
    try:
        # Reuters RSS feed approach
        rss_url = "https://feeds.reuters.com/reuters/businessNews"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(rss_url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.content, 'xml')
        
        articles = []
        items = soup.find_all('item')
        
        for item in items[:10]:  # Limit to 10 articles
            title = item.find('title')
            link = item.find('link')
            description = item.find('description')
            
            if title and link:
                articles.append({
                    'title': title.get_text(strip=True) if title else '',
                    'url': link.get_text(strip=True) if link else '',
                    'description': description.get_text(strip=True) if description else '',
                    'source': 'Reuters'
                })
        
        logging.info(f"Reuters RSS: Found {len(articles)} articles")
        return articles
    except Exception as e:
        logging.error(f"Error fetching Reuters news: {e}")
        return []

def fetch_marketwatch_rss():
    """
    Fetch news from MarketWatch RSS feed (No API key needed)
    """
    try:
        rss_url = "https://feeds.content.dowjones.io/public/rss/mw_topstories"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(rss_url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.content, 'xml')
        
        articles = []
        items = soup.find_all('item')[:10]
        
        for item in items:
            title = item.find('title')
            link = item.find('link')
            description = item.find('description')
            
            if title and link:
                articles.append({
                    'title': title.get_text(strip=True),
                    'url': link.get_text(strip=True),
                    'description': description.get_text(strip=True) if description else '',
                    'source': 'MarketWatch'
                })
        
        logging.info(f"MarketWatch RSS: Found {len(articles)} articles")
        return articles
    except Exception as e:
        logging.error(f"Error fetching MarketWatch news: {e}")
        return []

def fetch_cnbc_rss():
    """
    Fetch news from CNBC RSS feed (No API key needed)
    """
    try:
        rss_url = "https://www.cnbc.com/id/100003114/device/rss/rss.html"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(rss_url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.content, 'xml')
        
        articles = []
        items = soup.find_all('item')[:10]
        
        for item in items:
            title = item.find('title')
            link = item.find('link')
            description = item.find('description')
            
            if title and link:
                articles.append({
                    'title': title.get_text(strip=True),
                    'url': link.get_text(strip=True),
                    'description': description.get_text(strip=True) if description else '',
                    'source': 'CNBC'
                })
        
        logging.info(f"CNBC RSS: Found {len(articles)} articles")
        return articles
    except Exception as e:
        logging.error(f"Error fetching CNBC news: {e}")
        return []

def fetch_bloomberg_rss():
    """
    Fetch news from Bloomberg RSS feed (No API key needed)
    """
    try:
        rss_url = "https://feeds.bloomberg.com/markets/news.rss"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(rss_url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.content, 'xml')
        
        articles = []
        items = soup.find_all('item')[:10]
        
        for item in items:
            title = item.find('title')
            link = item.find('link')
            description = item.find('description')
            
            if title and link:
                articles.append({
                    'title': title.get_text(strip=True),
                    'url': link.get_text(strip=True),
                    'description': description.get_text(strip=True) if description else '',
                    'source': 'Bloomberg'
                })
        
        logging.info(f"Bloomberg RSS: Found {len(articles)} articles")
        return articles
    except Exception as e:
        logging.error(f"Error fetching Bloomberg news: {e}")
        return []

def get_ticker_sector_info(ticker):
    """
    Get sector and related keywords for a ticker to fetch relevant economic news.
    """
    sector_mapping = {
        # Technology
        'TSLA': {'sector': 'Automotive/EV', 'keywords': 'automotive OR electric vehicles OR EV OR Tesla OR car sales OR auto industry OR battery OR charging'},
        'AAPL': {'sector': 'Technology', 'keywords': 'Apple OR iPhone OR smartphone OR tech earnings OR semiconductor OR AI OR software'},
        'NVDA': {'sector': 'Technology/Semiconductor', 'keywords': 'Nvidia OR AI chip OR semiconductor OR GPU OR data center OR gaming'},
        'MSFT': {'sector': 'Technology', 'keywords': 'Microsoft OR Azure OR Office OR cloud computing OR enterprise software'},
        'GOOGL': {'sector': 'Technology', 'keywords': 'Google OR Alphabet OR search OR advertising OR YouTube OR cloud OR AI'},
        'META': {'sector': 'Technology/Social Media', 'keywords': 'Meta OR Facebook OR social media OR advertising OR VR OR metaverse'},
        'AMZN': {'sector': 'Technology/Retail', 'keywords': 'Amazon OR e-commerce OR AWS OR cloud OR retail OR logistics'},
        'AMD': {'sector': 'Technology/Semiconductor', 'keywords': 'AMD OR processor OR CPU OR GPU OR semiconductor OR data center'},
        'INTC': {'sector': 'Technology/Semiconductor', 'keywords': 'Intel OR processor OR chip manufacturing OR foundry OR semiconductor'},
        
        # Healthcare/Biotech
        'JNJ': {'sector': 'Healthcare', 'keywords': 'Johnson Johnson OR healthcare OR pharmaceuticals OR medical devices OR vaccines'},
        'PFE': {'sector': 'Healthcare/Pharma', 'keywords': 'Pfizer OR pharmaceutical OR vaccine OR drug approval OR healthcare'},
        'UNH': {'sector': 'Healthcare/Insurance', 'keywords': 'UnitedHealth OR health insurance OR Medicare OR healthcare services'},
        'ABBV': {'sector': 'Healthcare/Pharma', 'keywords': 'AbbVie OR pharmaceutical OR drug OR biotech OR healthcare'},
        
        # Financial
        'JPM': {'sector': 'Financial/Banking', 'keywords': 'JPMorgan OR banking OR financial services OR interest rates OR Fed OR lending'},
        'BAC': {'sector': 'Financial/Banking', 'keywords': 'Bank America OR banking OR financial services OR interest rates OR Fed'},
        'WFC': {'sector': 'Financial/Banking', 'keywords': 'Wells Fargo OR banking OR financial services OR lending OR mortgage'},
        'GS': {'sector': 'Financial/Investment', 'keywords': 'Goldman Sachs OR investment banking OR trading OR financial services'},
        
        # Energy
        'XOM': {'sector': 'Energy/Oil', 'keywords': 'Exxon OR oil OR energy OR crude oil OR natural gas OR refinery OR drilling'},
        'CVX': {'sector': 'Energy/Oil', 'keywords': 'Chevron OR oil OR energy OR crude oil OR natural gas OR refinery'},
        'COP': {'sector': 'Energy/Oil', 'keywords': 'ConocoPhillips OR oil OR energy OR crude oil OR natural gas OR exploration'},
        
        # Consumer
        'KO': {'sector': 'Consumer/Beverages', 'keywords': 'Coca-Cola OR beverage OR soft drinks OR consumer goods OR retail'},
        'PEP': {'sector': 'Consumer/Beverages', 'keywords': 'Pepsi OR beverage OR soft drinks OR snacks OR consumer goods'},
        'WMT': {'sector': 'Consumer/Retail', 'keywords': 'Walmart OR retail OR consumer spending OR grocery OR e-commerce'},
        'PG': {'sector': 'Consumer/Goods', 'keywords': 'Procter Gamble OR consumer goods OR household OR personal care OR retail'},
        
        # Industrial
        'BA': {'sector': 'Industrial/Aerospace', 'keywords': 'Boeing OR aerospace OR aircraft OR defense OR aviation OR airline'},
        'CAT': {'sector': 'Industrial/Machinery', 'keywords': 'Caterpillar OR construction OR machinery OR infrastructure OR mining'},
        'GE': {'sector': 'Industrial/Conglomerate', 'keywords': 'General Electric OR industrial OR aviation OR healthcare OR power'},
        
        # Communication
        'VZ': {'sector': 'Communication/Telco', 'keywords': 'Verizon OR telecommunications OR 5G OR wireless OR internet'},
        'T': {'sector': 'Communication/Telco', 'keywords': 'AT T OR telecommunications OR 5G OR wireless OR streaming'},
        
        # Utilities
        'NEE': {'sector': 'Utilities/Renewable', 'keywords': 'NextEra Energy OR renewable energy OR solar OR wind OR utilities'},
        'SO': {'sector': 'Utilities', 'keywords': 'Southern Company OR utilities OR electricity OR power generation'},
        
        # Real Estate
        'PLD': {'sector': 'Real Estate/REIT', 'keywords': 'Prologis OR real estate OR warehouse OR logistics OR REIT'},
        
        # Materials
        'LIN': {'sector': 'Materials/Chemicals', 'keywords': 'Linde OR chemicals OR industrial gases OR materials OR manufacturing'},
        
        # Crypto-related
        'COIN': {'sector': 'Financial/Crypto', 'keywords': 'Coinbase OR cryptocurrency OR Bitcoin OR crypto trading OR digital assets'},
        'MSTR': {'sector': 'Financial/Crypto', 'keywords': 'MicroStrategy OR Bitcoin OR cryptocurrency OR digital assets OR crypto investment'},
    }
    
    # Check if we have specific mapping for this ticker
    if ticker in sector_mapping:
        return sector_mapping[ticker]
    
    # Default fallback based on common patterns
    ticker_upper = ticker.upper()
    if any(keyword in ticker_upper for keyword in ['TECH', 'SOFT', 'NET', 'SYS', 'DATA']):
        return {'sector': 'Technology', 'keywords': 'technology OR software OR IT OR digital OR innovation'}
    elif any(keyword in ticker_upper for keyword in ['BIO', 'PHARMA', 'HEALTH', 'MED']):
        return {'sector': 'Healthcare', 'keywords': 'healthcare OR pharmaceutical OR medical OR biotech OR drug'}
    elif any(keyword in ticker_upper for keyword in ['BANK', 'FIN', 'CREDIT']):
        return {'sector': 'Financial', 'keywords': 'banking OR financial services OR lending OR investment OR insurance'}
    elif any(keyword in ticker_upper for keyword in ['OIL', 'GAS', 'ENERGY', 'FUEL']):
        return {'sector': 'Energy', 'keywords': 'energy OR oil OR gas OR renewable OR power OR utility'}
    elif any(keyword in ticker_upper for keyword in ['AUTO', 'CAR', 'MOTOR', 'VEHICLE']):
        return {'sector': 'Automotive', 'keywords': 'automotive OR car sales OR auto industry OR transportation OR vehicle'}
    else:
        # Generic sector keywords
        return {'sector': 'General Market', 'keywords': 'stock market OR earnings OR economy OR financial markets OR investment'}

def fetch_comprehensive_news(api_key, ticker=None, use_fallback=True):
    """
    Fetch news from multiple sources with timeout protection.
    If use_fallback=True, falls back to NewsAPI only for reliability.
    """
    all_articles = []
    
    try:
        # 1. NewsAPI (primary source - most reliable)
        if ticker:
            # Multiple query attempts for better ticker-specific results
            newsapi_query = f'"{ticker}" OR "{ticker} stock" OR "{ticker} earnings" OR "${ticker}"'
            logging.info(f"NewsAPI query for ticker {ticker}: {newsapi_query}")
        else:
            newsapi_query = "economy OR stock market OR inflation OR fed OR interest rates OR GDP OR unemployment OR earnings OR central bank OR monetary policy"
            logging.info(f"NewsAPI query for general news: {newsapi_query}")
        
        newsapi_articles = fetch_economic_news(api_key, query=newsapi_query, page_size=20)
        
        if newsapi_articles:
            # Filter for ticker-specific articles if ticker is provided
            if ticker:
                ticker_filtered = []
                ticker_lower = ticker.lower()
                
                for article in newsapi_articles:
                    title = (article.get('title') or '').lower()
                    desc = (article.get('description') or '').lower()
                    
                    # Check if ticker appears in title or description
                    if ticker_lower in title or ticker_lower in desc:
                        article['source'] = 'NewsAPI'
                        ticker_filtered.append(article)
                
                logging.info(f"NewsAPI: Found {len(newsapi_articles)} articles, {len(ticker_filtered)} are ticker-specific")
                
                # Use filtered articles if we have them, otherwise use all
                if ticker_filtered:
                    all_articles.extend(ticker_filtered)
                    if len(ticker_filtered) >= 5:
                        logging.info(f"Using {len(ticker_filtered)} ticker-specific NewsAPI articles")
                        return ticker_filtered[:15]
                else:
                    logging.warning(f"No ticker-specific articles found for {ticker}, using general articles")
                    for article in newsapi_articles:
                        article['source'] = 'NewsAPI'
                    all_articles.extend(newsapi_articles)
            else:
                for article in newsapi_articles:
                    article['source'] = 'NewsAPI'
                all_articles.extend(newsapi_articles)
                logging.info(f"NewsAPI: Found {len(newsapi_articles)} articles for general news")
        else:
            logging.warning(f"NewsAPI returned 0 articles for query: {newsapi_query}")
        
    except Exception as e:
        logging.error(f"NewsAPI failed: {e}", exc_info=True)
        logging.info("Continuing with fallback news sources...")
    
    # 2. Free RSS Feeds (no API key needed - always try these)
    # Yahoo Finance
    try:
        yahoo_articles = fetch_yahoo_finance_news(ticker)
        all_articles.extend(yahoo_articles)
    except Exception as e:
        logging.error(f"Yahoo Finance failed: {e}")
    
    # 3. Reuters RSS
    try:
        reuters_articles = fetch_reuters_news()
        all_articles.extend(reuters_articles)
    except Exception as e:
        logging.error(f"Reuters failed: {e}")
    
    # 4. MarketWatch RSS (Free, no API key)
    try:
        marketwatch_articles = fetch_marketwatch_rss()
        all_articles.extend(marketwatch_articles)
    except Exception as e:
        logging.error(f"MarketWatch failed: {e}")
    
    # 5. CNBC RSS (Free, no API key)
    try:
        cnbc_articles = fetch_cnbc_rss()
        all_articles.extend(cnbc_articles)
    except Exception as e:
        logging.error(f"CNBC failed: {e}")
    
    # 6. Bloomberg RSS (Free, no API key)
    try:
        bloomberg_articles = fetch_bloomberg_rss()
        all_articles.extend(bloomberg_articles)
    except Exception as e:
        logging.error(f"Bloomberg failed: {e}")
    
    # If no articles found, return a fallback message
    if not all_articles:
        logging.warning("No news articles found from any source")
        return [{
            'title': 'No recent news available',
            'url': '',
            'description': 'Unable to fetch news from available sources',
            'source': 'System'
        }]
    
    # Remove duplicates based on title similarity
    unique_articles = []
    seen_titles = set()
    
    for article in all_articles:
        if not article.get('title'):
            continue
            
        title_lower = article['title'].lower()
        # Simple duplicate detection - check if similar title already exists
        is_duplicate = False
        for seen_title in seen_titles:
            if len(set(title_lower.split()) & set(seen_title.split())) > 3:  # 3+ common words
                is_duplicate = True
                break
        
        if not is_duplicate:
            unique_articles.append(article)
            seen_titles.add(title_lower)
    
    # Sort by recency (if available) and limit to top 15 for performance
    logging.info(f"Total unique articles: {len(unique_articles)}")
    return unique_articles[:15]

def classify_news(articles):
    """
    Classify news articles into Good, Bad, and Hidden Edge categories
    with improved filtering for financial relevance and sentiment analysis
    """
    good_keywords = ["growth", "record high", "bull", "bullish", "optimism", "beat", "strong", "rally", 
                    "positive", "surge", "gain", "profit", "earnings beat", "upgrade", "outperform",
                    "rises", "soars", "climbs", "advances", "rebounds", "recovery", "breakthrough",
                    "all-time high", "momentum", "strength", "confidence", "expansion"]
    
    bad_keywords = ["recession", "crash", "bear", "bearish", "drop", "decline", "miss", "weak", 
                   "negative", "downgrade", "loss", "selloff", "plunge", "tank", "tumbles",
                   "falls", "slumps", "sinks", "disappoints", "concerns", "risks", "threats",
                   "uncertainty", "volatility", "correction", "downturn", "fears", "worries"]
    
    hidden_keywords = ["unexpected", "surprise", "unnoticed", "quiet", "edge", "hidden", 
                      "under the radar", "overlooked", "emerging", "sleeper", "breakthrough",
                      "uncover", "discover", "beneath", "subtle", "stealth"]
    
    # Financial relevance keywords - article must contain at least one
    financial_keywords = ["stock", "market", "trading", "investor", "shares", "equity", "fund", 
                         "economy", "economic", "fed", "federal reserve", "treasury", "gdp", 
                         "earnings", "revenue", "profit", "wall street", "nasdaq", "s&p", 
                         "dow", "index", "sector", "etf", "portfolio", "analyst", "price target",
                         "central bank", "interest rate", "inflation", "employment"]

    good, bad, hidden = [], [], []
    
    for article in articles:
        title = (article.get("title") or "").lower()
        description = (article.get("description") or "").lower()
        content = f"{title} {description}"
        
        # Skip if not financially relevant
        if not any(keyword in content for keyword in financial_keywords):
            logging.debug(f"Skipping non-financial article: {title[:60]}")
            continue
        
        # Count keyword matches for better classification
        good_count = sum(1 for word in good_keywords if word in content)
        bad_count = sum(1 for word in bad_keywords if word in content)
        hidden_count = sum(1 for word in hidden_keywords if word in content)
        
        # Classify based on highest match count (handles mixed sentiment better)
        if hidden_count > 0:
            hidden.append(article)
            logging.info(f"Hidden Edge ({hidden_count} matches): {title[:60]}")
        elif good_count > bad_count:
            good.append(article)
            logging.info(f"Good News ({good_count} vs {bad_count}): {title[:60]}")
        elif bad_count > good_count:
            bad.append(article)
            logging.info(f"Bad News ({bad_count} vs {good_count}): {title[:60]}")
        else:
            # Neutral - use sentiment analysis as tiebreaker
            try:
                from textblob import TextBlob
                blob = TextBlob(content)
                if blob.sentiment.polarity > 0.1:
                    good.append(article)
                    logging.info(f"Good News (sentiment): {title[:60]}")
                elif blob.sentiment.polarity < -0.1:
                    bad.append(article)
                    logging.info(f"Bad News (sentiment): {title[:60]}")
            except:
                pass  # Skip if sentiment analysis fails
    
    logging.info(f"News classification: Good={len(good)}, Bad={len(bad)}, Hidden={len(hidden)}")
    return good, bad, hidden

@app.callback(
    Output('feature-outlier-scatter', 'figure'),
    Input('strategy-dropdown', 'value')
)
def update_feature_outlier_scatter(strategy):
    print(f"=== CALLBACK TRIGGERED: update_feature_outlier_scatter with strategy: {strategy} ===")
    print(f"=== DEBUG: This is the NEW callback version ===")
    logging.info(f"Callback triggered with strategy: {strategy}")
    
    if strategy not in STRATEGIES:
        logging.warning("Invalid strategy %s", strategy)
        return {"data": [], "layout": {"title": "Invalid strategy", "paper_bgcolor": "#222", "plot_bgcolor": "#222", "font": {"color": "#fff"}}}

    x_col, y_col, *_ = STRATEGIES[strategy]
    print(f"Strategy columns: x_col={x_col}, y_col={y_col}")

    try:
        # Debug database path
        import os
        db_path = os.path.abspath('billions.db')
        print(f"Database path: {db_path}")
        print(f"Database exists: {os.path.exists(db_path)}")
        
        # fetch from Postgres
        print(f"Querying database for strategy: {strategy}")
        with SessionLocal() as sess:
            # First check total count
            total_count = sess.query(PerfMetric).count()
            print(f"Total rows in database: {total_count}")
            
            rows = sess.query(PerfMetric).filter(PerfMetric.strategy == strategy).all()
            print(f"âœ“ Found {len(rows)} rows for strategy {strategy}")
            
            if len(rows) > 0:
                sample_row = rows[0]
                print(f"âœ“ Sample row: {sample_row.symbol} - metric_x: {sample_row.metric_x}, metric_y: {sample_row.metric_y}")

        if not rows:
            print("âœ— No rows found, returning empty plot")
            return {"data": [], "layout": {"title": "No data", "paper_bgcolor": "#222", "plot_bgcolor": "#222", "font": {"color": "#fff"}}}

        # Create DataFrame with valid column names
        df = pd.DataFrame([{
            'symbol': r.symbol,
            'metric_x': float(r.metric_x),
            'metric_y': float(r.metric_y)
        } for r in rows])
        df['Ticker'] = df['symbol']
        print(f"Created DataFrame with {len(df)} rows")
        
    except Exception as e:
        print(f"ERROR in database processing: {e}")
        logging.error(f"Database error: {e}")
        return {"data": [], "layout": {"title": f"Database error: {str(e)}", "paper_bgcolor": "#222", "plot_bgcolor": "#222", "font": {"color": "#fff"}}}

    # Create scatter plot using the actual column names
    fig = px.scatter(
        df,
        x='metric_x',
        y='metric_y',
        text='Ticker',
        title=f"Performance Scatter Plot ({strategy.capitalize()}): {x_col} vs {y_col}",
        template='none'
    )

    # Update plot styling
    fig.update_traces(
        marker=dict(
            size=12,
            color='#39FF14',
            line=dict(width=2, color='#fff')
        ),
        textposition='top center'
    )

    fig.update_layout(
        autosize=False,
        width=1200,
        height=700,
        paper_bgcolor='#222',
        plot_bgcolor='#222',
        font_color='#fff',
        xaxis=dict(
            color='#fff',
            title=f"{x_col} Performance",
            gridcolor='#444'
        ),
        yaxis=dict(
            color='#fff', 
            title=f"{y_col} Performance",
            gridcolor='#444'
        )
    )
    fig.update_yaxes(
        scaleanchor="x",
        scaleratio=1
    )

    # Log data for debugging
    logging.info(f"Successfully loaded data for {strategy} with {len(df)} rows")
    logging.info(f"Columns: {df.columns.tolist()}")
    logging.info(f"DataFrame shape: {df.shape}")
    logging.info(f"X column: {x_col}, Y column: {y_col}")
    print(f"First few rows of {strategy} data:")
    print(df.head())
    print(f"Scatter plot created with {len(df)} data points")
    
    # Test: Create a simple plot to verify data is valid
    print(f"Data ranges: metric_x={df['metric_x'].min():.2f} to {df['metric_x'].max():.2f}")
    print(f"Data ranges: metric_y={df['metric_y'].min():.2f} to {df['metric_y'].max():.2f}")
    print(f"=== RETURNING FIGURE TO DASH ===")

    return fig

# --- News/NLP/OTCMarkets Analysis Functions ---
def summarize_news(news_articles):
    # Simple extractive summary: join top 3 headlines
    if not news_articles:
        return "No recent news found for this ticker."
    summary = ' | '.join([a['title'] for a in news_articles[:3] if a.get('title')])
    return summary

def detect_negative_sentiment(news_articles):
    for a in news_articles:
        text = (a.get('title') or '') + ' ' + (a.get('description') or '')
        blob = TextBlob(text)
        if blob.sentiment.polarity < -0.2:
            return a['title']
    return None

def detect_hype(news_articles):
    """
    Detect hype/promotion in news articles with expanded keywords and better logging
    """
    hype_keywords = [
        'soars', 'explodes', 'must buy', 'promotion', 'pump', 'moon', 'hype', 
        'skyrockets', 'rockets', 'surges', 'to the moon', 'buy now', 'hot stock',
        'next big thing', 'explosive growth', 'monster gains', 'parabolic',
        'breakout alert', 'buy alert', 'stock alert', 'penny stock', 'millionaire maker',
        'get rich', 'limited time', 'act now', 'don\'t miss', 'massive potential'
    ]
    
    if not news_articles:
        logging.info("No news articles to check for hype")
        return None
    
    for i, a in enumerate(news_articles):
        title = (a.get('title') or '')
        description = (a.get('description') or '')
        text = (title + ' ' + description).lower()
        
        # Check each keyword
        for keyword in hype_keywords:
            if keyword in text:
                logging.warning(f"HYPE DETECTED in article {i+1}: keyword='{keyword}', title='{title[:100]}'")
                return title
    
    logging.info("No hype detected in news articles")
    return None

def check_otc_caveat_emptor(ticker):
    """
    Check if a ticker has Caveat Emptor or Prohibited status on OTCMarkets.com
    Returns: True if warning found, False if clean, error string if check failed
    """
    url = f"https://www.otcmarkets.com/stock/{ticker}/overview"
    
    try:
        logging.info(f"Checking OTC status for {ticker} at {url}")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        resp = requests.get(url, headers=headers, timeout=10)
        logging.info(f"OTC Markets response status: {resp.status_code}")
        
        if resp.status_code == 404:
            logging.info(f"{ticker} not found on OTC Markets (likely not an OTC stock)")
            return False
        
        if resp.status_code != 200:
            logging.warning(f"OTC Markets returned status {resp.status_code}")
            return f"OTC Markets check inconclusive (HTTP {resp.status_code})"
        
        soup = BeautifulSoup(resp.text, 'html.parser')
        page_text = soup.get_text()
        
        # Look for warning indicators
        warning_phrases = [
            'Caveat Emptor',
            'caveat emptor',
            'CAVEAT EMPTOR',
            'Prohibited',
            'prohibited',
            'PROHIBITED',
            'Shell Risk',
            'Unsolicited Quotes',
            'Grey Market'
        ]
        
        found_warnings = []
        for phrase in warning_phrases:
            if phrase in page_text:
                found_warnings.append(phrase)
        
        if found_warnings:
            logging.warning(f"âš ï¸ OTC WARNING DETECTED for {ticker}: {', '.join(found_warnings)}")
            return True
        
        logging.info(f"âœ“ No OTC warnings found for {ticker}")
        return False
        
    except requests.Timeout:
        logging.error(f"Timeout checking OTC Markets for {ticker}")
        return "OTC Markets check timed out"
    except requests.RequestException as e:
        logging.error(f"Network error checking OTC Markets for {ticker}: {e}")
        return f"Network error: {str(e)}"
    except Exception as e:
        logging.error(f"Error checking OTC Markets for {ticker}: {e}", exc_info=True)
        return f"Error checking OTCMarkets: {str(e)}"

# Outlier detection moved to refresh button - no startup blocking

# Refresh button callbacks
@app.callback(
    [Output("refresh-button", "disabled"),
     Output("refresh-status", "children"),
     Output("refresh-spinner", "children")],
    [Input("refresh-button", "n_clicks")],
    prevent_initial_call=True
)
def handle_refresh_click(n_clicks):
    if n_clicks:
        success = start_refresh_thread()
        if success:
            return True, "Refreshing data... Please wait", None
        else:
            return False, "Refresh already in progress", None
    return False, "Ready to refresh data", None

# Progress update callback
@app.callback(
    Output("refresh-status", "children", allow_duplicate=True),
    Input("interval-component", "n_intervals"),
    prevent_initial_call=True
)
def update_refresh_status(n):
    status = get_refresh_status()
    
    if status['is_running']:
        progress = status['progress']
        strategy = status['current_strategy']
        message = status['message']
        
        if status['estimated_completion']:
            remaining = int(status['estimated_completion'] - datetime.now().timestamp())
            if remaining > 0:
                message += f" (Est. {remaining}s remaining)"
        
        return f"ðŸ”„ {message} - {progress}% complete"
    else:
        return status['message']

# Run the app
if __name__ == '__main__':
    app.run(
        debug=True, 
        port=8050,
        host='127.0.0.1',
        dev_tools_hot_reload=False,  # Disable hot reload to prevent HTTP errors
        dev_tools_ui=False,          # Disable dev tools UI
        dev_tools_props_check=False, # Disable props check
        use_reloader=False           # Disable reloader to prevent connection issues
    )
